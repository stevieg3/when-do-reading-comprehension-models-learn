{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3df5043-baf6-4ed9-8d9c-67fe8b3888f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e1c663-cede-4a80-b629-67797bdcff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/notebooks/exploration'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6958f79-5de7-41eb-9cad-404e464cad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509c239-3d8f-4476-a49f-448a5f94f0ab",
   "metadata": {},
   "source": [
    "### MRQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e733e5f-c3c3-4d3f-b921-e0d1cf83ddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_CHECKPOINTS = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 812, 844, 876, 908, 940, 972, 1004, 1036, 1100, 1164, 1228, 1292, 1356, 1420, 1484, 1548, 1612, 1676, 1804, 1932, 2060, 2188, 2316, 2444, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 3980, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948]\n",
    "len(ALL_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12eb6b1f-33b2-4c5b-a9dd-845668463100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 bioasq 0\n",
      "27 drop 0\n",
      "27 duorc 0\n",
      "27 hotpotqa 0\n",
      "27 naturalquestions 0\n",
      "27 newsqa 0\n",
      "27 race 0\n",
      "27 relationextraction 0\n",
      "27 searchqa 0\n",
      "27 squad 0\n",
      "27 textbookqa 0\n",
      "27 triviaqa 0\n",
      "\n",
      "\n",
      "28 bioasq 0\n",
      "28 drop 0\n",
      "28 duorc 0\n",
      "28 hotpotqa 0\n",
      "28 naturalquestions 0\n",
      "28 newsqa 0\n",
      "28 race 0\n",
      "28 relationextraction 0\n",
      "28 searchqa 0\n",
      "28 squad 0\n",
      "28 textbookqa 0\n",
      "28 triviaqa 0\n",
      "\n",
      "\n",
      "29 bioasq 0\n",
      "29 drop 0\n",
      "29 duorc 0\n",
      "29 hotpotqa 0\n",
      "29 naturalquestions 0\n",
      "29 newsqa 0\n",
      "29 race 0\n",
      "29 relationextraction 0\n",
      "29 searchqa 0\n",
      "29 squad 0\n",
      "29 textbookqa 0\n",
      "29 triviaqa 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_dict = {}\n",
    "\n",
    "for seed in [27, 28, 29]:\n",
    "    for dataset in ['bioasq', 'drop', 'duorc', 'hotpotqa', 'naturalquestions', 'newsqa', 'race', 'relationextraction', 'searchqa', 'squad', 'textbookqa', 'triviaqa']: \n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-mrqa-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-dataset={dataset}')\n",
    "        chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "\n",
    "        remaining_chkpts = set(ALL_CHECKPOINTS) - set(chkpts)\n",
    "\n",
    "        remaining_chkpts_dict[(seed, dataset)] = np.sort(list(remaining_chkpts))\n",
    "        print(seed, dataset, len(remaining_chkpts_dict[(seed, dataset)]))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60521da-8dcd-4079-ba8a-f3be1b8d2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in remaining_chkpts_dict.items():\n",
    "    if len(remaining_chkpts_dict[k]) != 0:\n",
    "        print(k[0], '-', k[1], '-', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4232b2ca-0f08-4713-b358-ea5f6e724a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for seed in [27, 28, 29]:\n",
    "    for dataset in ['bioasq', 'drop', 'duorc', 'hotpotqa', 'naturalquestions', 'newsqa', 'race', 'relationextraction', 'searchqa', 'squad', 'textbookqa', 'triviaqa']: \n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-mrqa-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-dataset={dataset}')\n",
    "\n",
    "        for chkpt in chkpts:\n",
    "            if len(os.listdir(f'predictions/albert-xlarge-v2-mrqa-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-dataset={dataset}/{chkpt}')) < 4:\n",
    "                print(seed, '-', dataset, '-', chkpt)\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09942f48-d6ba-4b1a-8720-8794e64da3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'812, 844, 876, 908, 940, 972, 1004, 1036, 1100, 1164, 1228, 1292, 1356, 1420, 1484, 1548, 1612, 1676, 1804, 1932, 2060, 2188, 2316, 2444, 2700, 3212, 5644, 6284, 7180, 7820, 3596, 3724, 3980, 4236, 4620, 5132, 5388, 6412, 6796, 6924, 7436, 3084, 3340, 3468, 4108, 5004, 5260, 5516, 5772, 5900, 6028, 6156, 6540, 7948, 3852, 2956, 4364, 6668, 7052, 7308, 2828, 4876, 4492, 4748'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x.split('-')[1] for x in chpt_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24033485-62f3-4ca8-86a0-71ca11c2daa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[30]:\n",
    "    string += str(num)\n",
    "    string += ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f24a0c9-2bbb-41cc-93c1-023380406eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 8, 10, 12, 14, 16, 20, 28, 32, 36, 52, 60, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 348, 380, 428, 460, 492, 524, 556, 588, 620, 684, 716, 780, 812, 844, 908, 940, 972, 1036, 1100, 1228, 1292, 1420, 1484, 1548, 1612, 1676, 1804, 2060, 2316, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948, '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45fecd99-d6aa-4080-a8f8-3b6275013e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chpts = []\n",
    "for dataset in ['bioasq', 'drop', 'duorc', 'hotpotqa', 'naturalquestions', 'newsqa', 'race', 'relationextraction', 'searchqa', 'squad', 'textbookqa', 'triviaqa']:\n",
    "    all_chpts += list(remaining_chkpts_dict[(29, dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14f976fd-0da4-459e-bc6c-e512a4285bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8049b29c-4465-4bed-baa8-0566d26a9e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_chpts)) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0f7639c-2e29-4490-9c6a-87c2f79dca77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 2572, 7564, 7692, '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in np.sort(list(set(all_chpts))):\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b806d6-ef5c-419f-bc70-c1572d8b14db",
   "metadata": {},
   "source": [
    "### SQuAD-AdversarialQA evaluation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf377e00-ba0a-4f97-8107-d3822b402bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_CHECKPOINTS = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 812, 844, 876, 908, 940, 972, 1004, 1036, 1100, 1164, 1228, 1292, 1356, 1420, 1484, 1548, 1612, 1676, 1804, 1932, 2060, 2188, 2316, 2444, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 3980, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948]\n",
    "len(ALL_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1e7bca-2613-44e9-8aa6-e5831f1f2c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0\n",
      "\n",
      "\n",
      "28 0\n",
      "\n",
      "\n",
      "29 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_dict = {}\n",
    "\n",
    "for seed in [27, 28, 29]:\n",
    "    chkpts = os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}')\n",
    "    chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "\n",
    "    remaining_chkpts = set(ALL_CHECKPOINTS) - set(chkpts)\n",
    "\n",
    "    remaining_chkpts_dict[seed] = np.sort(list(remaining_chkpts))\n",
    "    print(seed, len(remaining_chkpts_dict[seed]))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e59cdf-f1bb-4ecf-9987-b7b92df84cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpts = os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29')\n",
    "\n",
    "for chkpt in chkpts:\n",
    "    if len(os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/{chkpt}')) < 2:\n",
    "        print(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0af6270a-cf63-4c1d-921b-bdd3c5d6a4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[28]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4302e7f-2480-4929-835a-a3615ae42132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[29]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67110670-9219-478b-8a28-873ceb8d5622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test $SGE_TASK_ID -eq 1 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-1 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-1 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=1.log 2>&1\n",
      "test $SGE_TASK_ID -eq 2 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-10 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-10 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=10.log 2>&1\n",
      "test $SGE_TASK_ID -eq 3 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-14 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-14 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=14.log 2>&1\n",
      "test $SGE_TASK_ID -eq 4 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-124 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-124 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=124.log 2>&1\n"
     ]
    }
   ],
   "source": [
    "seed = 29\n",
    "for i, checkpoint in enumerate([1, 10, 14, 124]):\n",
    "    print(f'test $SGE_TASK_ID -eq {i+1} && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{checkpoint} --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{checkpoint} --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-checkpoint={checkpoint}.log 2>&1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032f778-a1c2-4300-8468-c447dca2f54c",
   "metadata": {},
   "source": [
    "### SQuAD-AdversarialQA evaluation on dev data (full outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4e4798-d15a-45a9-907d-c3ad597b855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_CHECKPOINTS = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 812, 844, 876, 908, 940, 972, 1004, 1036, 1100, 1164, 1228, 1292, 1356, 1420, 1484, 1548, 1612, 1676, 1804, 1932, 2060, 2188, 2316, 2444, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 3980, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948]\n",
    "len(ALL_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfd559b-1d3b-4755-a6ed-721f2deb99c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 dbert 0\n",
      "\n",
      "\n",
      "28 dbert 0\n",
      "\n",
      "\n",
      "29 dbert 0\n",
      "\n",
      "\n",
      "27 dbidaf 0\n",
      "\n",
      "\n",
      "28 dbidaf 0\n",
      "\n",
      "\n",
      "29 dbidaf 0\n",
      "\n",
      "\n",
      "27 droberta 0\n",
      "\n",
      "\n",
      "28 droberta 0\n",
      "\n",
      "\n",
      "29 droberta 0\n",
      "\n",
      "\n",
      "27 squad 0\n",
      "\n",
      "\n",
      "28 squad 0\n",
      "\n",
      "\n",
      "29 squad 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_dict = {}\n",
    "\n",
    "for dataset in ['dbert', 'dbidaf', 'droberta', 'squad']:\n",
    "    for seed in [27, 28, 29]:\n",
    "        chkpts = os.listdir(f'predictions/full/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{dataset}')\n",
    "        chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "\n",
    "        remaining_chkpts = set(ALL_CHECKPOINTS) - set(chkpts)\n",
    "\n",
    "        remaining_chkpts_dict[(seed, dataset)] = np.sort(list(remaining_chkpts))\n",
    "        print(seed, dataset, len(remaining_chkpts_dict[(seed, dataset)]))\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6512b02-9e2e-440a-b52b-af8b809d109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#=================================#\n",
      "(27, 'dbert')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(28, 'dbert')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(29, 'dbert')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(27, 'dbidaf')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(28, 'dbidaf')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(29, 'dbidaf')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(27, 'droberta')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(28, 'droberta')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(29, 'droberta')\n",
      "\n",
      "Total:  0\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(27, 'squad')\n",
      "7052,7436,7820\n",
      "Total:  3\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(28, 'squad')\n",
      "3340,3468,3596,3980,4108,4236,4364,4492,4620,4748,5004,5260,5388,5516,5644,5772,6028,6156,6284,6412,6540,6668,7820,7948\n",
      "Total:  24\n",
      "\n",
      "\n",
      "#=================================#\n",
      "(29, 'squad')\n",
      "3340,3468,3596,3724,4108,4236,4492,4748,4876,5132,5260,5516,5644,5900,6028,6156,6284,6412,6540,6668,6796,6924,7308,7692,7820\n",
      "Total:  25\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in remaining_chkpts_dict.items():\n",
    "    print('#=================================#')\n",
    "    print(k)\n",
    "    print(','.join([str(i) for i in v]))\n",
    "    print('Total: ', len(v))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7df578ef-95d7-430d-83d0-812756a52faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(np.concatenate(list(remaining_chkpts_dict.values())))) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588fa36e-020d-4f6d-9fdc-95326c5712da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.concatenate(list(remaining_chkpts_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a949b1b-82dd-4b71-8f96-53f4c6e0ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad 27 checkpoint-1484\n",
      "squad 28 checkpoint-1484\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['dbert', 'dbidaf', 'droberta', 'squad']:\n",
    "    for seed in [27, 28, 29]:\n",
    "        chkpts = os.listdir(f'predictions/full/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{dataset}')\n",
    "\n",
    "        for chkpt in chkpts:\n",
    "            if len(os.listdir(f'predictions/full/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{dataset}/{chkpt}')) < 5:\n",
    "                print(dataset, seed, chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2171334-ff47-4552-a9fa-4186eca0b085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[28]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e0f36e-59a8-4219-9399-a7932c85dbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[29]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c4a4b57-52f6-4600-9a55-c965bc781391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test $SGE_TASK_ID -eq 1 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-1 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-1 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=1.log 2>&1\n",
      "test $SGE_TASK_ID -eq 2 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-10 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-10 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=10.log 2>&1\n",
      "test $SGE_TASK_ID -eq 3 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-14 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-14 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=14.log 2>&1\n",
      "test $SGE_TASK_ID -eq 4 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-124 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-124 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=124.log 2>&1\n"
     ]
    }
   ],
   "source": [
    "seed = 29\n",
    "for i, checkpoint in enumerate([1, 10, 14, 124]):\n",
    "    print(f'test $SGE_TASK_ID -eq {i+1} && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{checkpoint} --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{checkpoint} --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-checkpoint={checkpoint}.log 2>&1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355734d8-dfad-4153-832f-695f46bd26e9",
   "metadata": {},
   "source": [
    "### AdversarialQA evaluation on CheckList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570d7108-e233-4c93-a4f2-e62407f7df9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_CHECKPOINTS = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 812, 844, 876, 908, 940, 972, 1004, 1036, 1100, 1164, 1228, 1292, 1356, 1420, 1484, 1548, 1612, 1676, 1804, 1932, 2060, 2188, 2316, 2444, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 3980, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948]\n",
    "len(ALL_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf62e4f-0e7e-4a23-83b3-942e85bb7728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 0\n",
      "\n",
      "\n",
      "29 0\n",
      "\n",
      "\n",
      "30 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_dict = {}\n",
    "\n",
    "for seed in [28, 29, 30]:\n",
    "    chkpts = os.listdir(f'predictions/checklist/albert-xlarge-v2-adversarial_qa_all-wu=100-lr=3e5-bs=32-msl=384-seed={seed}')\n",
    "    chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "\n",
    "    remaining_chkpts = set(ALL_CHECKPOINTS) - set(chkpts)\n",
    "\n",
    "    remaining_chkpts_dict[seed] = np.sort(list(remaining_chkpts))\n",
    "    print(seed, len(remaining_chkpts_dict[seed]))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a0e6d5-eeff-4509-a1f0-82707d56164d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpts = os.listdir(f'predictions/checklist/albert-xlarge-v2-adversarial_qa_all-wu=100-lr=3e5-bs=32-msl=384-seed=30')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for chkpt in chkpts:\n",
    "    if len(os.listdir(f'predictions/checklist/albert-xlarge-v2-adversarial_qa_all-wu=100-lr=3e5-bs=32-msl=384-seed=30/{chkpt}')) < 4:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d8ad63-d6a6-4500-a696-bbb0235a9740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[28]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3539632f-6991-4e4e-8381-052854445e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[29]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb266039-62cd-450b-9943-d4cc1f3aa988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for num in remaining_chkpts_dict[30]:\n",
    "    string += str(num)\n",
    "    string += ', '\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11464b15-eed9-41a5-83c6-11e4e6d194a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test $SGE_TASK_ID -eq 1 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-8 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-8 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=8.log 2>&1\n",
      "test $SGE_TASK_ID -eq 2 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-12 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-12 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=12.log 2>&1\n",
      "test $SGE_TASK_ID -eq 3 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-28 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-28 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=28.log 2>&1\n",
      "test $SGE_TASK_ID -eq 4 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-44 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-44 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=44.log 2>&1\n",
      "test $SGE_TASK_ID -eq 5 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-76 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-76 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=76.log 2>&1\n",
      "test $SGE_TASK_ID -eq 6 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-92 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-92 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=92.log 2>&1\n",
      "test $SGE_TASK_ID -eq 7 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-108 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-108 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=108.log 2>&1\n",
      "test $SGE_TASK_ID -eq 8 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2316 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2316 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=2316.log 2>&1\n",
      "test $SGE_TASK_ID -eq 9 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2444 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2444 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=2444.log 2>&1\n",
      "test $SGE_TASK_ID -eq 10 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2572 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2572 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=2572.log 2>&1\n",
      "test $SGE_TASK_ID -eq 11 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2700 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2700 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=2700.log 2>&1\n",
      "test $SGE_TASK_ID -eq 12 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2828 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2828 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=2828.log 2>&1\n",
      "test $SGE_TASK_ID -eq 13 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2956 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-2956 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=2956.log 2>&1\n",
      "test $SGE_TASK_ID -eq 14 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3084 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3084 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3084.log 2>&1\n",
      "test $SGE_TASK_ID -eq 15 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3212 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3212 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3212.log 2>&1\n",
      "test $SGE_TASK_ID -eq 16 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3340 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3340 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3340.log 2>&1\n",
      "test $SGE_TASK_ID -eq 17 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3468 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3468 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3468.log 2>&1\n",
      "test $SGE_TASK_ID -eq 18 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3596 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3596 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3596.log 2>&1\n",
      "test $SGE_TASK_ID -eq 19 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3724 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3724 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3724.log 2>&1\n",
      "test $SGE_TASK_ID -eq 20 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3852 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3852 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3852.log 2>&1\n",
      "test $SGE_TASK_ID -eq 21 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3980 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-3980 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=3980.log 2>&1\n",
      "test $SGE_TASK_ID -eq 22 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4108 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4108 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4108.log 2>&1\n",
      "test $SGE_TASK_ID -eq 23 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4236 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4236 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4236.log 2>&1\n",
      "test $SGE_TASK_ID -eq 24 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4364 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4364 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4364.log 2>&1\n",
      "test $SGE_TASK_ID -eq 25 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4492 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4492 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4492.log 2>&1\n",
      "test $SGE_TASK_ID -eq 26 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4620 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4620 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4620.log 2>&1\n",
      "test $SGE_TASK_ID -eq 27 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4748 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4748 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4748.log 2>&1\n",
      "test $SGE_TASK_ID -eq 28 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4876 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-4876 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=4876.log 2>&1\n",
      "test $SGE_TASK_ID -eq 29 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5004 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5004 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5004.log 2>&1\n",
      "test $SGE_TASK_ID -eq 30 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5132 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5132 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5132.log 2>&1\n",
      "test $SGE_TASK_ID -eq 31 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5260 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5260 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5260.log 2>&1\n",
      "test $SGE_TASK_ID -eq 32 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5388 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5388 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5388.log 2>&1\n",
      "test $SGE_TASK_ID -eq 33 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5516 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5516 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5516.log 2>&1\n",
      "test $SGE_TASK_ID -eq 34 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5644 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5644 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5644.log 2>&1\n",
      "test $SGE_TASK_ID -eq 35 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5772 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5772 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5772.log 2>&1\n",
      "test $SGE_TASK_ID -eq 36 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5900 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-5900 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=5900.log 2>&1\n",
      "test $SGE_TASK_ID -eq 37 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6028 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6028 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6028.log 2>&1\n",
      "test $SGE_TASK_ID -eq 38 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6156 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6156 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6156.log 2>&1\n",
      "test $SGE_TASK_ID -eq 39 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6284 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6284 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6284.log 2>&1\n",
      "test $SGE_TASK_ID -eq 40 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6412 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6412 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6412.log 2>&1\n",
      "test $SGE_TASK_ID -eq 41 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6540 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6540 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6540.log 2>&1\n",
      "test $SGE_TASK_ID -eq 42 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6668 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6668 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6668.log 2>&1\n",
      "test $SGE_TASK_ID -eq 43 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6796 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6796 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6796.log 2>&1\n",
      "test $SGE_TASK_ID -eq 44 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6924 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-6924 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=6924.log 2>&1\n",
      "test $SGE_TASK_ID -eq 45 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7052 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7052 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7052.log 2>&1\n",
      "test $SGE_TASK_ID -eq 46 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7180 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7180 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7180.log 2>&1\n",
      "test $SGE_TASK_ID -eq 47 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7308 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7308 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7308.log 2>&1\n",
      "test $SGE_TASK_ID -eq 48 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7436 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7436 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7436.log 2>&1\n",
      "test $SGE_TASK_ID -eq 49 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7564 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7564 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7564.log 2>&1\n",
      "test $SGE_TASK_ID -eq 50 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7692 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7692 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7692.log 2>&1\n",
      "test $SGE_TASK_ID -eq 51 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7820 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7820 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7820.log 2>&1\n",
      "test $SGE_TASK_ID -eq 52 && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7948 --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/checkpoint-7948 --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29-checkpoint=7948.log 2>&1\n"
     ]
    }
   ],
   "source": [
    "seed = 29\n",
    "for i, checkpoint in enumerate([8, 12, 28, 44, 76, 92, 108, 2316, 2444, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 3980, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948]):\n",
    "    print(f'test $SGE_TASK_ID -eq {i+1} && sleep 10 && python src/models/run_qa.py --model_name_or_path /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/models/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{checkpoint} --do_predict --test_file /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/data/processed/squad1_dadversarialQA_1_1_weighted_train.json --per_device_eval_batch_size 64 --output_dir /SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn/predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{checkpoint} --overwrite_output_dir --overwrite_cache --report_to none > logs/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-checkpoint={checkpoint}.log 2>&1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e3f10-bdb8-405f-a3c9-41b28373f377",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5b336-8bcc-4199-a6c8-2ed1056e8437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c4409-1f5d-4c86-bb10-984303471366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b870f7-0345-4411-8a1c-641f674e0331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75c396-da30-4652-8f94-869d02987a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d14f3803-aa82-4ee3-a411-62f1f41dd8d5",
   "metadata": {},
   "source": [
    "SQuAD v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31227a8e-07d2-487a-8d0a-98467eaebd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [27, 29, 30]:\n",
    "    chkpts = os.listdir(f'predictions/albert-xlarge-v2-squadv2-wu=100-lr=3e5-bs=32-msl=384-seed={seed}')\n",
    "c\n",
    "    for chkpt in chkpts:\n",
    "        if len(os.listdir(f'predictions/albert-xlarge-v2-squadv2-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/{chkpt}')) != 5:\n",
    "            print(seed, chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c516be5-30ab-47eb-ae60-ab2de8af4ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0380c3-ad62-4aad-84d4-878c37061399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498a1b1-9283-41fb-b4c6-a391b324b6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45449664-3d46-404a-829d-fa4ee410ad74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQ_ADQA_CHECKPOINTS = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 812]\n",
    "len(SQ_ADQA_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c656b791-36c9-425c-bde4-21ae95eaded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 set()\n",
      "28 set()\n",
      "29 {32, 36, 68, 8, 10, 12, 44, 14, 76, 92, 108, 52, 28}\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_list = []\n",
    "\n",
    "for seed in [27, 28, 29]:\n",
    "    chkpts = os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}')\n",
    "    chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "    \n",
    "    remaining_chkpts = set(SQ_ADQA_CHECKPOINTS) - set(chkpts)\n",
    "    remaining_chkpts_list.append(remaining_chkpts)\n",
    "    \n",
    "#     chkpts = np.sort(chkpts)\n",
    "#     print(seed, chkpts[-1])\n",
    "    print(seed, remaining_chkpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960c3445-5659-43fc-9a06-8537d1d3a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-5\n",
      "checkpoint-1\n",
      "checkpoint-20\n",
      "checkpoint-124\n"
     ]
    }
   ],
   "source": [
    "chkpts = os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29')\n",
    "\n",
    "for chkpt in chkpts:\n",
    "    if len(os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=29/{chkpt}')) < 2:\n",
    "        print(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315b28d1-60e3-4489-b929-99d610929e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_chkpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c2c4b-8072-4e8a-bb33-969de984aa69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9ba53-e376-4571-a040-380f451a8f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29220caf-5055-4c7c-8dca-46a7889c3e9e",
   "metadata": {},
   "source": [
    "SQuAD v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad5125-b24c-4aa0-995b-137ecbd72e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11913a83-8840-4d8b-8654-9d3ee8f597f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae3db3-6556-489e-9993-3b98a3ceb365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be2d54-16c6-4a7a-971d-dbcbb2c013b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bbcd7-23e4-4d6d-b61f-79d08da67161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27b4409-fe61-4b0c-8cfc-96013847b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['dbert', 'dbidaf', 'droberta']:\n",
    "    for seed in [28, 29, 30]:\n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-squadv1-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}')\n",
    "\n",
    "        for chkpt in chkpts:\n",
    "            if len(os.listdir(f'predictions/albert-xlarge-v2-squadv1-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/{chkpt}')) != 4:\n",
    "                print(model, seed, chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac51d66-d0e1-429f-ae89-b39e81fd5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['dbert', 'dbidaf', 'droberta']:\n",
    "    for seed in [28, 29, 30]:\n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-squadv1-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}')\n",
    "        chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "\n",
    "        remaining_chkpts = set(ALL_CHECKPOINTS) - set(chkpts)\n",
    "        \n",
    "        if len(remaining_chkpts) != 0:\n",
    "            print(model, seed, remaining_chkpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ae749-db2c-46df-8f48-d3f75cf37818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a19301-46b5-41ac-96c7-49f055452c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{380, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7820, 7948},\n",
       " {380,\n",
       "  6284,\n",
       "  6412,\n",
       "  6540,\n",
       "  6668,\n",
       "  6796,\n",
       "  6924,\n",
       "  7052,\n",
       "  7180,\n",
       "  7308,\n",
       "  7564,\n",
       "  7692,\n",
       "  7820,\n",
       "  7948},\n",
       " {6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7692, 7820, 7948},\n",
       " {6156, 6284, 6412, 6540, 6668, 6796, 6924, 7180, 7308, 7692, 7820, 7948},\n",
       " {6156, 6284, 6412, 6540, 6668, 6796, 6924, 7180, 7436, 7692, 7820, 7948},\n",
       " {6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7692, 7820, 7948}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_chkpts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7d64de-e0fe-4702-97d6-b352a50734a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7948,\n",
       " 6540,\n",
       " 7052,\n",
       " 6412,\n",
       " 6924,\n",
       " 6668,\n",
       " 7180,\n",
       " 6284,\n",
       " 6796,\n",
       " 7308,\n",
       " 7436,\n",
       " 7564,\n",
       " 7692,\n",
       " 7820,\n",
       " 380,\n",
       " 6156]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_chkpts_list_union = list(set().union(*remaining_chkpts_list))\n",
    "remaining_chkpts_list_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1771b777-218d-440d-91e7-d143a9cc31f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_chkpts_list_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ba2808-699e-40ec-9aa1-fec8d2596490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "108 in remaining_chkpts_list_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e47675fd-e7ca-401f-99e4-eac033ff0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_chkpts_list_union.append(108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5337c967-24a1-40fb-bc86-005f0d13eaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7948,\n",
       " 6540,\n",
       " 7052,\n",
       " 6412,\n",
       " 6924,\n",
       " 6668,\n",
       " 7180,\n",
       " 6284,\n",
       " 6796,\n",
       " 7308,\n",
       " 7436,\n",
       " 7564,\n",
       " 7692,\n",
       " 7820,\n",
       " 380,\n",
       " 6156,\n",
       " 108]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_chkpts_list_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda8bf50-c16b-4e4a-bd66-70d760a0537b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_chkpts_list_union)  * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0337e-2de4-4229-9e16-98b97890138d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2904a7-5670-4a8f-a848-caf20876acae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86a2012-d36a-4d0b-833d-46a8766506cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_list = []\n",
    "\n",
    "for seed in [27, 28, 29]:\n",
    "    chkpts = os.listdir(f'predictions/on-train/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-1')\n",
    "    chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "    \n",
    "    print(len(chkpts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52eea752-07f6-43df-80bb-d8abba9ed7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for num in (set(ALL_CHECKPOINTS) - set(chkpts)):\n",
    "    string += str(num)\n",
    "    string += ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edfcb968-8e7f-4606-9943-5f8eeaaa70dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2188, 6284, 2316, 2444, 3212, 4236, 3596, 7564, 6924, 2572, 3084, 6412, 2060, 7436, 2700, 2828, 6796, 7052, 2956, 7308, 3340, 3468, 3724, 7820, 7948, 3852, 3980, 4108, 4364, 6668, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5772, 5900, 6028, 6156, '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e2c19-5b4a-49fe-94c6-c9855dfc382b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0792600e-c2e1-4b1f-8c38-e4a7e968cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpts = os.listdir(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=28')\n",
    "\n",
    "for chkpt in chkpts:\n",
    "    if len(os.listdir(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=28/{chkpt}')) < 4:\n",
    "        print(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1defaf4-087d-4515-a3ab-6fff1950a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "60, 32, 68, 36, 44, 76, 92, 2572, 4748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9beb454-6aba-4cdf-a53c-fb8c7a33c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7b35d-2cd1-4394-a896-1b016cc2b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b5850c3-3720-4b19-9aa9-48b186f46c91",
   "metadata": {},
   "source": [
    "seed 30 - checkpoint 16\n",
    "seed 29 - 6028, 6412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2408e-477c-4c3c-91c8-b3a7383f652f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7577940d-8b53-47f1-a7a8-b025373bc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c756e6-9db7-4e54-bea6-083aee918ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0b517-4d7d-46c7-85b6-91b5761eae51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0004b6-f42a-47d6-ab37-f2371e886e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f5f295-7609-4251-86ff-3adf3130bf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27-dbert\n",
      "71\n",
      "\n",
      "\n",
      "28-dbert\n",
      "71\n",
      "\n",
      "\n",
      "29-dbert\n",
      "71\n",
      "\n",
      "\n",
      "27-dbidaf\n",
      "71\n",
      "\n",
      "\n",
      "28-dbidaf\n",
      "71\n",
      "\n",
      "\n",
      "29-dbidaf\n",
      "71\n",
      "\n",
      "\n",
      "27-droberta\n",
      "71\n",
      "\n",
      "\n",
      "28-droberta\n",
      "71\n",
      "\n",
      "\n",
      "29-droberta\n",
      "71\n",
      "\n",
      "\n",
      "27-squad\n",
      "70\n",
      "\n",
      "\n",
      "28-squad\n",
      "70\n",
      "\n",
      "\n",
      "29-squad\n",
      "70\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_list = []\n",
    "missing_checkpoints = []\n",
    "\n",
    "for dataset in ['dbert', 'dbidaf', 'droberta', 'squad']:\n",
    "    for seed in [27, 28, 29]:\n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{dataset}')\n",
    "        chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "        \n",
    "        missing = list(set(ALL_CHECKPOINTS) - set(chkpts))\n",
    "        missing_checkpoints += missing\n",
    "        \n",
    "        print(f'{seed}-{dataset}')\n",
    "        print(len(chkpts))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "978f4bfe-f667-4a47-995b-0bebd7f4f370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(missing_checkpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45edfee7-7a23-43c2-879b-8e199c45c025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for num in (set(missing_checkpoints)):\n",
    "    string += str(num)\n",
    "    string += ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a19b1d9d-cf38-41f3-aa3a-a7420db23272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2956, 7052, 6412, 2444, 6540, 6284, 2316, 2188, 3212, 4236, 6924, 2700, 2828, 6796, 3084, 7180, 7308, 7436, 3340, 7564, 3468, 3596, 7692, 3724, 7820, 7948, 3852, 3980, 4108, 2572, 4364, 6668, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 6028, 5388, 5516, 5644, 5772, 1676, 1804, 5900, 6156, 1932, 2060, '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a80ee4-e6d8-45f6-ac74-9f04f57b156f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdafda1-cf56-48a3-8bd6-f17a96e762e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2de8dd-cdc1-45e8-bc5a-bdfb692b16c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36dd683b-c37b-4134-a742-c5f329cd036b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for num in (set(ALL_CHECKPOINTS) - set(chkpts)):\n",
    "    string += str(num)\n",
    "    string += ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7985a016-355f-45af-81dd-69799f00646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2188, 6284, 2316, 2444, 3212, 4236, 3596, 7564, 6924, 2572, 3084, 6412, 2060, 7436, 2700, 2828, 6796, 7052, 2956, 7308, 3340, 3468, 3724, 7820, 7948, 3852, 3980, 4108, 4364, 6668, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5772, 5900, 6028, 6156, '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708729e5-48d8-4df7-984a-0a43e081f855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc377ab-394f-4d0d-a41d-8ae74fd13ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782dfeb-390f-420b-a616-6913d5a29d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984c6e0-91c5-4e2f-bef0-93f5f28df19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13347336-d705-482e-8539-e3a84b94baaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfc626-80d7-4009-b06f-97587f3fc2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618a1025-d37b-4b53-9099-18da8b814f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "remaining_chkpts_list = []\n",
    "\n",
    "for seed in [27, 28, 29]:\n",
    "    for model in ['dbert', 'dbidaf', 'droberta', 'squad']:\n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}')\n",
    "        chkpts = [int(i.split('-')[-1]) for i in chkpts]\n",
    "    \n",
    "        print(len(chkpts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae942f1-8bdb-4ff0-b484-663eef941b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for num in (set(ALL_CHECKPOINTS) - set(chkpts)):\n",
    "    string += str(num)\n",
    "    string += ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f527e3-6124-4232-9fb1-d7e405ccc8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2188, 6284, 2316, 2444, 3212, 4236, 3596, 7564, 6924, 2572, 3084, 6412, 2060, 7436, 2700, 2828, 6796, 7052, 2956, 7308, 3340, 3468, 3724, 7820, 7948, 3852, 3980, 4108, 4364, 6668, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5772, 5900, 6028, 6156, '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7defeb-09d4-49c1-9400-5cb02ed28774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4267b711-99ba-46e7-a75d-f4cded07baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [27, 28, 29]:\n",
    "    for model in ['dbert', 'dbidaf', 'droberta', 'squad']:\n",
    "        chkpts = os.listdir(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}')\n",
    "\n",
    "        for chkpt in chkpts:\n",
    "            if len(os.listdir(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}/{chkpt}')) < 4:\n",
    "                print(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}/{chkpt}')\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4ac49-f9c4-4e1a-b963-69b93c856769",
   "metadata": {},
   "outputs": [],
   "source": [
    "1484, 1548, 1612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b023179-6558-4e69-b633-430767517d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3786d-e512-4ec0-8ded-66a78429e3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689d2e9-f99d-4e8b-bab1-19baa49b5414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff16ac7-15bf-4e12-9817-2d154440941d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aee7a9-45a2-4926-abf1-148fa68e2350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8689982-73d0-4dcb-886c-f8e85629cfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156ed24-c59b-4d7c-a79d-1722df2450ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b97676-e576-4a1d-a83b-cbf7d0faca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [27, 28, 29]:\n",
    "    for chkpt in ALL_CHECKPOINTS:\n",
    "        try:\n",
    "            pd.read_csv(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{chkpt}/checklist_results_summary.txt')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "        for chkpt in chkpts:\n",
    "            if len(os.listdir(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}/{chkpt}')) < 4:\n",
    "                print(f'predictions/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}-{model}/{chkpt}')\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c8ad3-cbb6-4280-8814-5a30d62fb5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffadb4-dfa6-4106-b770-3eed3d36584a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361560c8-d8fd-44a8-a062-4786d951e245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2a779-f8d5-4f41-9f12-0868ae1ea78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18fcd9e8-eb56-4bd2-8443-685673789c8f",
   "metadata": {},
   "source": [
    "## Context prediction checks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "46a8274b-631f-45eb-b771-3f00ec6324cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f16f7c9a-bd70-41f6-b7ee-6b229d43580b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/SAN/intelsys/rclearn/when-do-reading-comprehension-models-learn'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ff819cce-854c-4972-a262-2a0ca16bc3ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O\n",
      "P: of atmospheric oxygen levels show a global downward trend, because of fossil\n",
      "\n",
      "\n",
      "27 - 1 - 1\n",
      "Context: Breathing pure O\n",
      "P: 2 in space applications\n",
      "\n",
      "\n",
      "Context: Breathing pure O\n",
      "P: in some modern space suits, or in early spacecraft such as Apollo, causes no damage due to the low total pressures used\n",
      "\n",
      "\n",
      "27 - 8 - 2\n",
      "Context: Uptake of O\n",
      "P: medicine\n",
      "\n",
      "\n",
      "Context: Uptake of O\n",
      "P: respiration, so oxygen supplementation is used in medicine\n",
      "\n",
      "\n",
      "27 - 124 - 2\n",
      "Context: Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O\n",
      "P: ultraviolet radiation\n",
      "\n",
      "\n",
      "Context: Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O\n",
      "P: by ultraviolet radiation\n",
      "\n",
      "\n",
      "27 - 716 - 2\n",
      "Context: Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O\n",
      "P: ultraviolet radiation\n",
      "\n",
      "\n",
      "Context: Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O\n",
      "P: ultraviolet radiation impacting\n",
      "\n",
      "\n",
      "27 - 812 - 2\n",
      "Context: Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O\n",
      "P: downward trend\n",
      "\n",
      "\n",
      "Context: Aeolian sand with a number of dunes parted by peat swamps or small ponds cover the highest terrace. Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O\n",
      "P: downward\n",
      "\n",
      "\n",
      "27 - 1036 - 2\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: chemical\n",
      "\n",
      "\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: exothermic reaction\n",
      "\n",
      "\n",
      "Context: Increased O\n",
      "P: Evangelical Lutheran Church in Kenya\n",
      "\n",
      "\n",
      "27 - 2188 - 3\n",
      "Context: Many major classes of organic molecules in living organisms, such as proteins, nucleic acids, carbohydrates, and fats, contain oxygen, as do the major inorganic compounds that are constituents of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as it is a part of water, the major constituent of lifeforms. Oxygen is used in cellular respiration and released by photosynthesis, which uses the energy of sunlight to produce oxygen from water. It is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (O\n",
      "P: ozone (O\\n3), strongly absorbs UVB radiation and consequently the high-altitude ozone layer helps protect the biosphere from ultraviolet\n",
      "\n",
      "\n",
      "28 - 44 - 1\n",
      "Context: Kenya's first system of education was introduced by British colonists. After Kenya's independence on 12 December 1963, an authority named the Ominde Commission was formed to introduce changes that would reflect the nation's sovereignty. The commission focused on identity and unity, which were critical issues at the time. Changes in the subject content of history and geography were made to reflect national cohesion. Between 1964 and 1985, the 7423 system was adopted  seven years of primary, four years of lower secondary, two years of upper secondary, and three years of university. All schools had a common curriculum.\n",
      "P: i/njuksl. The three-stage Saturn V was designed to send a fully fueled CSM and LM to the Moon. It was 33 feet (10.1 m) in diameter and stood 363 feet (110.6 m) tall with its 96,800-pound (43,900 kg) lunar payload. Its capability grew to 103,600 pounds (47,000 kg) for the later advanced lunar landings. The S-IC first stage burned RP-1/LOX for a rated thrust of 7,500,000 pounds-force (33,400 kN), which was upgraded to 7,610,000 pounds-force (33,900 kN). The second and third stages burned liquid hydrogen, and the third stage was a modified version of the S-IVB, with thrust increased to 230,000 lbf (1,020 kN) and capability to restart the engine for translunar injection after reaching a parking orbit.\n",
      "\n",
      "\n",
      "Context: Increasingly, states have to give mutual recognition to each other's standards of regulation, while the EU has attempted to harmonise minimum ideals of best practice. Kenya's first system of education was introduced by British colonists. After Kenya's independence on 12 December 1963, an authority named the Ominde Commission was formed to introduce changes that would reflect the nation's sovereignty. The commission focused on identity and unity, which were critical issues at the time. Changes in the subject content of history and geography were made to reflect national cohesion. Between 1964 and 1985, the 7423 system was adopted  seven years of primary, four years of lower secondary, two years of upper secondary, and three years of university. All schools had a common curriculum.\n",
      "P: 33 feet (10.1 m\n",
      "\n",
      "\n",
      "Context: The Panthers finished the regular season with a 151 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 4915 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 124 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 2018 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.\n",
      "P: 1963\n",
      "\n",
      "\n",
      "Context: The Panthers finished the regular season with a 151 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 4915 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 124 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 2018 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.However he cannot do so until a 4-week period has elapsed, during which the Law Officers of the Scottish Government or UK Government can refer the bill to the Supreme Court of the United Kingdom for a ruling on whether it is within the powers of the Parliament. \n",
      "P: 12 December 1963\n",
      "\n",
      "\n",
      "Context: Richard Allen and Absalom Jones became the first African Americans ordained by the Methodist Church. They were licensed by St. George's Church in 1784. Three years later, protesting racial segregation in the worship services, Allen led most of the black members out of St. George's; eventually they founded the Mother Bethel A.M.E. Church and the African Methodist Episcopal denomination. Absalom Jones became an Episcopal priest. In 1836, the church's basement was excavated to make room for a Sunday School. In the 1920s a court case saved the church from being demolished to make way for the Benjamin Franklin Bridge. The case resulted in the bridge being relocated. Historic St Georges welcomes visitors and is home to archives and a museum on Methodism.\n",
      "P: quarterback Cam Newton\n",
      "\n",
      "\n",
      "Context: Matthew Lewis and Absalom Jones became the first African Americans ordained by the Methodist Church. They were licensed by St. George's Church in 1784. Three years later, protesting racial segregation in the worship services, Allen led most of the black members out of St. George's; eventually they founded the Mother Bethel A.M.E. Church and the African Methodist Episcopal denomination. Absalom Jones became an Episcopal priest. In 1836, the church's basement was excavated to make room for a Sunday School. In the 1920s a court case saved the church from being demolished to make way for the Benjamin Franklin Bridge. The case resulted in the bridge being relocated. Historic St Georges welcomes visitors and is home to archives and a museum on Methodism.\n",
      "P: quarterback Cam Newton\n",
      "\n",
      "\n",
      "Context: Concerns were raised over whether Levi's Stadium's field was of a high enough quality to host a Super Bowl; during the inaugural season, the field had to be re-sodded multiple times due to various issues, and during a week 6 game earlier in the 2015 season, a portion of the turf collapsed under Baltimore Ravens kicker Justin Tucker, causing him to slip and miss a field goal, although the field has not had any major issues since. As is customary for Super Bowl games played at natural grass stadiums, the NFL re-sodded the field with a new playing surface; a hybrid Bermuda 419 turf. NFL and Atlanta Braves field director Ed Mangan stated that the field was in \"great shape\" for gameday. However, the turf showed problem throughout the game, with a number of players needing to change their cleats during the game and player slipping during plays all throughout the game.\n",
      "P: St. George's\n",
      "\n",
      "\n",
      "Context: Concerns were raised over whether Levi's Stadium's field was of a high enough quality to host a Super Bowl; during the inaugural season, the field had to be re-sodded multiple times due to various issues, and during a week 6 game earlier in the 2015 season, a portion of the turf collapsed under Baltimore Ravens kicker Justin Tucker, causing him to slip and miss a field goal, although the field has not had any major issues since. As is customary for Super Bowl games played at natural grass stadiums, the NFL re-sodded the field with a new playing surface; a hybrid Bermuda 419 turf. NFL and Atlanta Braves field director James Morales stated that the field was in \"great shape\" for gameday. However, the turf showed problem throughout the game, with a number of players needing to change their cleats during the game and player slipping during plays all throughout the game.\n",
      "P: St. George's Church\n",
      "\n",
      "\n",
      "Context: Concerns were raised over whether Levi's Stadium's field was of a high enough quality to host a Super Bowl; during the inaugural season, the field had to be re-sodded multiple times due to various issues, and during a week 6 game earlier in the 2015 season, a portion of the turf collapsed under Baltimore Ravens kicker Justin Tucker, causing him to slip and miss a field goal, although the field has not had any major issues since. As is customary for Super Bowl games played at natural grass stadiums, the NFL re-sodded the field with a new playing surface; a hybrid Bermuda 419 turf. NFL and Atlanta Braves field director Christopher Gomez stated that the field was in \"great shape\" for gameday. However, the turf showed problem throughout the game, with a number of players needing to change their cleats during the game and player slipping during plays all throughout the game.\n",
      "P: Atlanta Braves field director Ed Mangan\n",
      "\n",
      "\n",
      "Context: Luther justified his opposition to the rebels on three grounds. First, in choosing violence over lawful submission to the secular government, they were ignoring Christ's counsel to \"Render unto Caesar the things that are Caesar's\"; St. Paul had written in his epistle to the Romans 13:17 that all authorities are appointed by God and therefore should not be resisted. This reference from the Bible forms the foundation for the doctrine known as the Divine Right of Kings, or, in the German case, the divine right of the princes. Second, the violent actions of rebelling, robbing, and plundering placed the peasants \"outside the law of God and Empire\", so they deserved \"death in body and soul, if only as highwaymen and murderers.\" Lastly, Luther charged the rebels with blasphemy for calling themselves \"Christian brethren\" and committing their sinful acts under the banner of the Gospel.\n",
      "P: James Morales\n",
      "\n",
      "\n",
      "Context: Luther justified his opposition to the rebels on three grounds. First, in choosing violence over lawful submission to the secular government, they were ignoring Luis's counsel to \"Render unto Caesar the things that are Caesar's\"; St. Paul had written in his epistle to the Romans 13:17 that all authorities are appointed by God and therefore should not be resisted. This reference from the Bible forms the foundation for the doctrine known as the Divine Right of Kings, or, in the German case, the divine right of the princes. Second, the violent actions of rebelling, robbing, and plundering placed the peasants \"outside the law of God and Empire\", so they deserved \"death in body and soul, if only as highwaymen and murderers.\" Lastly, Luther charged the rebels with blasphemy for calling themselves \"Christian brethren\" and committing their sinful acts under the banner of the Gospel.\n",
      "P: Christopher Gomez\n",
      "\n",
      "\n",
      "Context: As the designated home team in the annual rotation between AFC and NFC teams, the Broncos elected to wear their road white jerseys with matching white pants. Elway stated, \"We've had Super Bowl success in our white uniforms.\" The Broncos last wore matching white jerseys and pants in the Super Bowl in Super Bowl XXXIII, Elway's last game as Denver QB, when they defeated the Atlanta Falcons 3419. In their only other Super Bowl win in Super Bowl XXXII, Denver wore blue jerseys, which was their primary color at the time. They also lost Super Bowl XXI when they wore white jerseys, but they are 0-4 in Super Bowls when wearing orange jerseys, losing in Super Bowl XII, XXII, XXIV, and XLVIII. The only other AFC champion team to have worn white as the designated home team in the Super Bowl was the Pittsburgh Steelers; they defeated the Seattle Seahawks 2110 in Super Bowl XL 10 seasons prior. The Broncos' decision to wear white meant the Panthers would wear their standard home uniform: black jerseys with silver pants.\n",
      "P: death in body and soul\n",
      "\n",
      "\n",
      "Context: As the designated home team in the annual rotation between AFC and NFC teams, the Broncos elected to wear their road white jerseys with matching white pants. Elway stated, \"We've had Super Bowl success in our white uniforms.\" The Broncos last wore matching white jerseys and pants in the Super Bowl in Super Bowl XXXIII, Elway's last game as Casa Grande QB, when they defeated the Atlanta Falcons 3419. In their only other Super Bowl win in Super Bowl XXXII, Casa Grande wore blue jerseys, which was their primary color at the time. They also lost Super Bowl XXI when they wore white jerseys, but they are 0-4 in Super Bowls when wearing orange jerseys, losing in Super Bowl XII, XXII, XXIV, and XLVIII. The only other AFC champion team to have worn white as the designated home team in the Super Bowl was the Pittsburgh Steelers; they defeated the Seattle Seahawks 2110 in Super Bowl XL 10 seasons prior. The Broncos' decision to wear white meant the Panthers would wear their standard home uniform: black jerseys with silver pants.\n",
      "P: death in body and soul\n",
      "\n",
      "\n",
      "Context: The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night. The event was held on February 1, 2016 at SAP Center in San Jose. Alongside the traditional media availabilities, the event featured an opening ceremony with player introductions on a replica of the Golden Gate Bridge.\n",
      "P: white\n",
      "\n",
      "\n",
      "Context: The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night. The event was held on February 1, 2016 at SAP Center in Ormond Beach. Alongside the traditional media availabilities, the event featured an opening ceremony with player introductions on a replica of the Golden Gate Bridge.\n",
      "P: white\n",
      "\n",
      "\n",
      "Context: Originating as the Jama'at al-Tawhid wal-Jihad in 1999, it pledged allegiance to al-Qaeda in 2004, participated in the Iraqi insurgency that followed the March 2003 invasion of Iraq by Western forces, joined the fight in the Syrian Civil War beginning in March 2011, and was expelled from al-Qaeda in early 2014, (which complained of its failure to consult and \"notorious intransigence\"). The group gained prominence after it drove Iraqi government forces out of key cities in western Iraq in a 2014 offensive. The group is adept at social media, posting Internet videos of beheadings of soldiers, civilians, journalists and aid workers, and is known for its destruction of cultural heritage sites. The United Nations has held ISIL responsible for human rights abuses and war crimes, and Amnesty International has reported ethnic cleansing by the group on a \"historic scale\". The group has been designated a terrorist organisation by the United Nations, the European Union and member states, the United States, India, Indonesia, Turkey, Saudi Arabia, Syria and other countries.\n",
      "P: SAP Center in San Jose\n",
      "\n",
      "\n",
      "Context: Originating as the Jama'at al-Tawhid wal-Jihad in 1999, it pledged allegiance to al-Qaeda in 2004, participated in the Iraqi insurgency that followed the March 2003 invasion of Palau by Western forces, joined the fight in the Syrian Civil War beginning in March 2011, and was expelled from al-Qaeda in early 2014, (which complained of its failure to consult and \"notorious intransigence\"). The group gained prominence after it drove Iraqi government forces out of key cities in western Palau in a 2014 offensive. The group is adept at social media, posting Internet videos of beheadings of soldiers, civilians, journalists and aid workers, and is known for its destruction of cultural heritage sites. The United Nations has held ISIL responsible for human rights abuses and war crimes, and Amnesty International has reported ethnic cleansing by the group on a \"historic scale\". The group has been designated a terrorist organisation by the United Nations, the European Union and member states, the United States, India, Indonesia, Turkey, Saudi Arabia, Syria and other countries.\n",
      "P: SAP Center\n",
      "\n",
      "\n",
      "Context: Sara became a activist before Elizabeth did.\n",
      "P: human rights abuses and war crimes\n",
      "\n",
      "\n",
      "Context: Elizabeth became a activist after Sara did.\n",
      "P: human rights abuses and war crimes\n",
      "\n",
      "\n",
      "Context: Ashley became a adviser before Christina did.\n",
      "P: Sara\n",
      "\n",
      "\n",
      "Context: Christina became a adviser after Ashley did.\n",
      "P: Elizabeth\n",
      "\n",
      "\n",
      "Context: Rebecca became a advisor before Amy did.\n",
      "P: Ashley\n",
      "\n",
      "\n",
      "Context: Amy became a advisor after Rebecca did.\n",
      "P: Christina\n",
      "\n",
      "\n",
      "Context: Brandon is an adviser. Taylor is not.\n",
      "P: Rebecca\n",
      "\n",
      "\n",
      "Context: Jacob is an interpreter. Jose is an economist.\n",
      "P: Amy\n",
      "\n",
      "\n",
      "Context: Jose is an economist. Jacob is an interpreter.\n",
      "P: Brandon is an adviser. Taylor\n",
      "\n",
      "\n",
      "Context: Stephanie is an editor. Amy is an academic.\n",
      "P: Jacob\n",
      "\n",
      "\n",
      "Context: Amy is an academic. Stephanie is an editor.\n",
      "P: Jacob\n",
      "\n",
      "\n",
      "Context: Amy is a nurse. Ethan is an adviser.\n",
      "P: Amy is an academic. Stephanie\n",
      "\n",
      "\n",
      "Context: Ethan is an adviser. Amy is a nurse.\n",
      "P: Amy is an academic. Stephanie\n",
      "\n",
      "\n",
      "Context: Megan and Chad are friends. He is an interpreter, and she is an activist.\n",
      "P: Ethan is an adviser. Amy\n",
      "\n",
      "\n",
      "Context: Chad and Megan are friends. She is an activist, and he is an interpreter.\n",
      "P: Ethan is an adviser. Amy\n",
      "\n",
      "\n",
      "Context: Aaron and Samantha are friends. She is an auditor, and he is an accountant.\n",
      "P: Megan and Chad\n",
      "\n",
      "\n",
      "Context: Crystal and Henry are friends. He is a historian, and she is an educator.\n",
      "P: Chad and Megan\n",
      "\n",
      "\n",
      "Context: Crystal and Henry are friends. He is a historian, and she is an educator.\n",
      "P: Aaron and Samantha\n",
      "\n",
      "\n",
      "Context: Henry and Crystal are friends. She is an educator, and he is a historian.\n",
      "P: Crystal and Henry\n",
      "\n",
      "\n",
      "Context: Sarah and Austin are friends. His mom is a journalist.\n",
      "P: Crystal and Henry\n",
      "\n",
      "\n",
      "Context: Chad and Leah are friends. Her mom is an advisor.\n",
      "P: Henry\n",
      "\n",
      "\n",
      "Context: Connor and Laura are friends. Her mom is an architect.\n",
      "P: Sarah and Austin\n",
      "\n",
      "\n",
      "Context: Richard and Mark are friends. The former is an entrepreneur.\n",
      "P: Chad and Leah\n",
      "\n",
      "\n",
      "Context: Mark and Richard are friends. The latter is an entrepreneur.\n",
      "P: Connor and Laura\n",
      "\n",
      "\n",
      "Context: Andrea and Jamie are friends. The former is an organizer.\n",
      "P: Mark and Richard\n",
      "\n",
      "\n",
      "Context: Jamie and Andrea are friends. The latter is an organizer.\n",
      "P: Richard and Mark\n",
      "\n",
      "\n",
      "Context: Sarah and Megan are friends. The former is an investigator.\n",
      "P: Jamie and Andrea\n",
      "\n",
      "\n",
      "Context: Megan and Sarah are friends. The latter is an investigator.\n",
      "P: Andrea and Jamie\n",
      "\n",
      "\n",
      "Context: Jacob prefers Michelle.\n",
      "P: Megan and Sarah\n",
      "\n",
      "\n",
      "Context: Michelle is preferred by Jacob.\n",
      "P: Sarah and Megan\n",
      "\n",
      "\n",
      "Context: Eric attacks Amanda.\n",
      "P: Jacob prefers Michelle\n",
      "\n",
      "\n",
      "Context: Amanda is attacked by Eric.\n",
      "P: Michelle is preferred by Jacob\n",
      "\n",
      "\n",
      "Context: Stephanie understands Taylor.\n",
      "P: Eric\n",
      "\n",
      "\n",
      "Context: Taylor is understood by Stephanie.\n",
      "P: Amanda is attacked by Eric\n",
      "\n",
      "\n",
      "Context: Sarah recognizes Thomas. Thomas recognizes Taylor.\n",
      "P: Stephanie understands Taylor\n",
      "\n",
      "\n",
      "Context: Sarah recognizes Thomas. Thomas recognizes Taylor.\n",
      "P: Stephanie\n",
      "\n",
      "\n",
      "Context: Sarah recognizes Thomas. Taylor is recognized by Thomas.\n",
      "P: Sarah recognizes Thomas. Thomas\n",
      "\n",
      "\n",
      "Context: Rebecca is accepted by Matthew. Rebecca accepts Olivia.\n",
      "P: Sarah recognizes Thomas. Thomas recognizes Taylor\n",
      "\n",
      "\n",
      "Context: Matthew understands Rachel. Rachel understands Mark.\n",
      "P: Sarah recognizes Thomas. Taylor\n",
      "\n",
      "\n",
      "Context: Rachel is understood by Matthew. Rachel understands Mark.\n",
      "P: Matthew understands Rachel. Rachel understands Mark\n",
      "\n",
      "\n",
      "28 - 220 - 57\n",
      "Context: Committees comprise a small number of MSPs, with membership reflecting the balance of parties across Parliament. There are different committees with their functions set out in different ways. Mandatory Committees are committees which are set down under the Scottish Parliament's standing orders, which govern their remits and proceedings. The current Mandatory Committees in the fourth Session of the Scottish Parliament are: Public Audit; Equal Opportunities; European and External Relations; Finance; Public Petitions; Standards, Procedures and Public Appointments; and Delegated Powers and Law Reform.\n",
      "P: i/njuksl. The three-stage Saturn V was designed to send a fully fueled CSM and LM to the Moon. It was 33 feet (10.1 m) in diameter and stood 363 feet (110.6 m) tall with its 96,800-pound (43,900 kg) lunar payload. Its capability grew to 103,600 pounds (47,000 kg) for the later advanced lunar landings. The S-IC first stage burned RP-1/LOX for a rated thrust of 7,500,000 pounds-force (33,400 kN), which was upgraded to 7,610,000 pounds-force (33,900 kN). The second and third stages burned liquid hydrogen, and the third stage was a modified version of the S-IVB, with thrust increased to 230,000 lbf (1,020 kN) and capability to restart the engine for translunar injection after reaching a parking orbit.\n",
      "\n",
      "\n",
      "Context: This is when chloroplast constriction begins.\n",
      "P: 33\n",
      "\n",
      "\n",
      "Context: The collection of Italian, Medieval, Renaissance, Baroque and Neoclassical sculpture (both original and in cast form) is unequalled outside of Italy. It includes Canova's The Three Graces, which the museum jointly owns with National Galleries of Scotland. Italian sculptors whose work is held by the museum include: Bartolomeo Bon, Bartolomeo Bellano, Luca della Robbia, Giovanni Pisano, Donatello, Agostino di Duccio, Andrea Riccio, Antonio Rossellino, Andrea del Verrocchio, Antonio Lombardo, Pier Jacopo Alari Bonacolsi, Andrea della Robbia, Michelozzo di Bartolomeo, Michelangelo (represented by a freehand wax model and casts of his most famous sculptures), Jacopo Sansovino, Alessandro Algardi, Antonio Calcagni, Benvenuto Cellini (Medusa's head dated c. 1547), Agostino Busti, Bartolomeo Ammannati, Giacomo della Porta, Giambologna (Samson Slaying a Philistine (Giambologna) c. 1562, his finest work outside Italy), Bernini (Neptune and Triton c. 16223), Giovanni Battista Foggini, Vincenzo Foggini (Samson and the Philistines), Massimiliano Soldani Benzi, Antonio Corradini, Andrea Brustolon, Giovanni Battista Piranesi, Innocenzo Spinazzi, Canova, Carlo Marochetti and Raffaelle Monti. An unusual sculpture is the ancient Roman statue of Narcissus restored by Valerio Cioli c1564 with plaster. There are several small scale bronzes by Donatello, Alessandro Vittoria, Tiziano Aspetti and Francesco Fanelli in the collection. The largest item from Italy is the Chancel Chapel from Santa Chiara Florence dated 14931500, designed by Giuliano da Sangallo it is 11.1 metres in height by 5.4 metres square, it includes a grand sculpted tabernacle by Antonio Rossellino and coloured terracotta decoration.\n",
      "P: the balance of parties across Parliament\n",
      "\n",
      "\n",
      "Context: The collection of Italian, Medieval, Renaissance, Baroque and Neoclassical sculpture (both original and in cast form) is unequalled outside of Italy. It includes Canova's The Three Graces, which the museum jointly owns with National Galleries of Scotland. Italian sculptors whose work is held by the museum include: Bartolomeo Bon, Bartolomeo Bellano, Luca della Robbia, Giovanni Pisano, Donatello, Agostino di Duccio, Andrea Riccio, Antonio Rossellino, Andrea del Verrocchio, Antonio Lombardo, Pier Jacopo Alari Bonacolsi, Andrea della Robbia, Michelozzo di Bartolomeo, Michelangelo (represented by a freehand wax model and casts of his most famous sculptures), Jacopo Sansovino, Alessandro Algardi, Antonio Calcagni, Benvenuto Cellini (Medusa's head dated c. 1547), Agostino Busti, Bartolomeo Ammannati, Giacomo della Porta, Giambologna (Aiden Slaying a Philistine (Giambologna) c. 1562, his finest work outside Italy), Bernini (Neptune and Triton c. 16223), Giovanni Battista Foggini, Vincenzo Foggini (Aiden and the Philistines), Massimiliano Soldani Benzi, Antonio Corradini, Andrea Brustolon, Giovanni Battista Piranesi, Innocenzo Spinazzi, Canova, Carlo Marochetti and Raffaelle Monti. An unusual sculpture is the ancient Roman statue of Narcissus restored by Valerio Cioli c1564 with plaster. There are several small scale bronzes by Donatello, Alessandro Vittoria, Tiziano Aspetti and Francesco Fanelli in the collection. The largest item from Italy is the Chancel Chapel from Santa Chiara Florence dated 14931500, designed by Giuliano da Sangallo it is 11.1 metres in height by 5.4 metres square, it includes a grand sculpted tabernacle by Antonio Rossellino and coloured terracotta decoration.\n",
      "P: balance of parties across Parliament\n",
      "\n",
      "\n",
      "Context: Since the 2005 revival, the Doctor generally travels with a primary female companion, who occupies a larger narrative role. Steven Moffat described the companion as the main character of the show, as the story begins anew with each companion and she undergoes more change than the Doctor. The primary companions of the Ninth and Tenth Doctors were Rose Tyler (Billie Piper), Martha Jones (Freema Agyeman), and Donna Noble (Catherine Tate) with Mickey Smith (Noel Clarke) and Jack Harkness (John Barrowman) recurring as secondary companion figures. The Eleventh Doctor became the first to travel with a married couple, Amy Pond (Karen Gillan) and Rory Williams (Arthur Darvill), whilst out-of-sync meetings with River Song (Alex Kingston) and Clara Oswald (Jenna Coleman) provided ongoing story arcs. The tenth series will introduce Pearl Mackie as Bill, the Doctor's newest traveling companion.\n",
      "P: Three Graces\n",
      "\n",
      "\n",
      "Context: Since the 2005 revival, the Doctor generally travels with a primary female companion, who occupies a larger narrative role. Steven Moffat described the companion as the main character of the show, as the story begins anew with each companion and she undergoes more change than the Doctor. The primary companions of the Ninth and Tenth Doctors were Rose Tyler (Billie Piper), Martha Jones (Freema Agyeman), and Donna Noble (Catherine Tate) with Mickey Smith (Noel Clarke) and Christopher Sanders (John Barrowman) recurring as secondary companion figures. The Eleventh Doctor became the first to travel with a married couple, Amy Pond (Karen Gillan) and Rory Williams (Arthur Darvill), whilst out-of-sync meetings with River Song (Alex Kingston) and Clara Oswald (Jenna Coleman) provided ongoing story arcs. The tenth series will introduce Pearl Mackie as Bill, the Doctor's newest traveling companion.\n",
      "P: The Three Graces\n",
      "\n",
      "\n",
      "Context: Despite the disagreements on the Eucharist, the Marburg Colloquy paved the way for the signing in 1530 of the Augsburg Confession, and for the formation of the Schmalkaldic League the following year by leading Protestant nobles such as John of Saxony, Philip of Hesse, and George, Margrave of Brandenburg-Ansbach. The Swiss cities, however, did not sign these agreements.\n",
      "P: Mickey Smith (Noel Clarke) and Christopher Sanders\n",
      "\n",
      "\n",
      "Context: Despite the disagreements on the Eucharist, the Marburg Colloquy paved the way for the signing in 1530 of the Augsburg Confession, and for the formation of the Schmalkaldic League the following year by leading Protestant nobles such as John of Saxony, Philip of Hesse, and Henry, Margrave of Brandenburg-Ansbach. The Swiss cities, however, did not sign these agreements.\n",
      "P: Mickey Smith (Noel Clarke) and Michael Martinez\n",
      "\n",
      "\n",
      "Context: Citizenship of the EU has increasingly been seen as a \"fundamental\" status of member state nationals by the Court of Justice, and has accordingly increased the number of social services that people can access wherever they move. The Court has required that higher education, along with other forms of vocational training, should be more access, albeit with qualifying periods. In Commission v Austria the Court held that Austria was not entitled to restrict places in Austrian universities to Austrian students to avoid \"structural, staffing and financial problems\" if (mainly German) foreign students applied for places because there was little evidence of an actual problem.\n",
      "P: George, Margrave of Brandenburg-Ansbach\n",
      "\n",
      "\n",
      "Context: Citizenship of the EU has increasingly been seen as a \"fundamental\" status of member state nationals by the Court of Justice, and has accordingly increased the number of social services that people can access wherever they move. The Court has required that higher education, along with other forms of vocational training, should be more access, albeit with qualifying periods. In Commission v Sint Maarten (Dutch part) the Court held that Sint Maarten (Dutch part) was not entitled to restrict places in Austrian universities to Austrian students to avoid \"structural, staffing and financial problems\" if (mainly German) foreign students applied for places because there was little evidence of an actual problem.\n",
      "P: Henry\n",
      "\n",
      "\n",
      "Context: Immigrants arrived from all over the world to search for gold, especially from Ireland and China. Many Chinese miners worked in Victoria, and their legacy is particularly strong in Bendigo and its environs. Although there was some racism directed at them, there was not the level of anti-Chinese violence that was seen at the Lambing Flat riots in New South Wales. However, there was a riot at Buckland Valley near Bright in 1857. Conditions on the gold fields were cramped and unsanitary; an outbreak of typhoid at Buckland Valley in 1854 killed over 1,000 miners.\n",
      "P: Commission v Austria\n",
      "\n",
      "\n",
      "Context: Immigrants arrived from all over the world to search for gold, especially from Ireland and China. Many Chinese miners worked in Palm Springs, and their legacy is particularly strong in Bendigo and its environs. Although there was some racism directed at them, there was not the level of anti-Chinese violence that was seen at the Lambing Flat riots in New South Wales. However, there was a riot at Buckland Valley near Bright in 1857. Conditions on the gold fields were cramped and unsanitary; an outbreak of typhoid at Buckland Valley in 1854 killed over 1,000 miners.\n",
      "P: Commission v Sint Maarten\n",
      "\n",
      "\n",
      "Context: The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 134849, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.[citation needed]\n",
      "P: Palm Springs\n",
      "\n",
      "\n",
      "Context: The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Tonga, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 134849, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.[citation needed]\n",
      "P: Bendigo and its environs\n",
      "\n",
      "\n",
      "Context: Samantha became a activist before Michelle did.\n",
      "P: port's trade with Constantinople, and ports on the Black Sea\n",
      "\n",
      "\n",
      "Context: Michelle became a activist after Samantha did.\n",
      "P: port's trade with Constantinople, and ports on the Black Sea\n",
      "\n",
      "\n",
      "Context: Nathan became a adviser before Stephanie did.\n",
      "P: Samantha\n",
      "\n",
      "\n",
      "Context: Stephanie became a adviser after Nathan did.\n",
      "P: Michelle\n",
      "\n",
      "\n",
      "Context: Thomas became a intern before James did.\n",
      "P: Nathan\n",
      "\n",
      "\n",
      "Context: James became a intern after Thomas did.\n",
      "P: Stephanie\n",
      "\n",
      "\n",
      "Context: Ethan is an author. Jessica is an intern.\n",
      "P: Thomas\n",
      "\n",
      "\n",
      "Context: Jose is an advisor. Scott is an auditor.\n",
      "P: James\n",
      "\n",
      "\n",
      "Context: Jose is an advisor. Scott is an auditor.\n",
      "P: Jessica\n",
      "\n",
      "\n",
      "Context: Scott is an auditor. Jose is an advisor.\n",
      "P: Jose is an advisor. Scott\n",
      "\n",
      "\n",
      "Context: Zachary is a historian. Samantha is a photographer.\n",
      "P: Scott\n",
      "\n",
      "\n",
      "Context: Savannah and Eric are friends. He is a photographer, and she is an analyst.\n",
      "P: Jose\n",
      "\n",
      "\n",
      "Context: Eric and Savannah are friends. She is an analyst, and he is a photographer.\n",
      "P: Samantha\n",
      "\n",
      "\n",
      "Context: Natalie and Scott are friends. He is an author, and she is an architect.\n",
      "P: Savannah and Eric\n",
      "\n",
      "\n",
      "Context: Scott and Natalie are friends. She is an architect, and he is an author.\n",
      "P: Eric\n",
      "\n",
      "\n",
      "Context: Jamie and Jonathan are friends. He is a producer, and she is an administrator.\n",
      "P: Natalie and Scott\n",
      "\n",
      "\n",
      "Context: Jonathan and Jamie are friends. She is an administrator, and he is a producer.\n",
      "P: Scott\n",
      "\n",
      "\n",
      "Context: Crystal and Joshua are friends. His mom is an actor.\n",
      "P: Jamie and Jonathan\n",
      "\n",
      "\n",
      "Context: Sara and Bryan are friends. His mom is an educator.\n",
      "P: Jonathan\n",
      "\n",
      "\n",
      "Context: Derek and Tracy are friends. His mom is an organizer.\n",
      "P: Crystal\n",
      "\n",
      "\n",
      "Context: Tracy and Derek are friends. His mom is an organizer.\n",
      "P: Sara\n",
      "\n",
      "\n",
      "Context: Emma and Michelle are friends. The former is an administrator.\n",
      "P: Derek and Tracy\n",
      "\n",
      "\n",
      "Context: Michelle and Emma are friends. The latter is an administrator.\n",
      "P: Tracy\n",
      "\n",
      "\n",
      "Context: Michelle and Emma are friends. The former is an engineer and the latter is an administrator.\n",
      "P: Emma and Michelle\n",
      "\n",
      "\n",
      "Context: Emma and Eric are friends. The former is an artist.\n",
      "P: Michelle and Emma\n",
      "\n",
      "\n",
      "Context: Eric and Emma are friends. The latter is an artist.\n",
      "P: Michelle and Emma\n",
      "\n",
      "\n",
      "Context: Maria and Rebecca are friends. The latter is an educator.\n",
      "P: Emma and Eric\n",
      "\n",
      "\n",
      "Context: Heather hurts Kelly.\n",
      "P: Eric and Emma\n",
      "\n",
      "\n",
      "Context: Kelly is hurt by Heather.\n",
      "P: Maria and Rebecca\n",
      "\n",
      "\n",
      "Context: Christian follows Thomas.\n",
      "P: Heather hurts Kelly\n",
      "\n",
      "\n",
      "Context: Thomas is followed by Christian.\n",
      "P: Kelly\n",
      "\n",
      "\n",
      "Context: Thomas is followed by Christian.\n",
      "P: Christian follows Thomas\n",
      "\n",
      "\n",
      "Context: Anthony is preferred by Benjamin.\n",
      "P: Thomas is followed by Christian\n",
      "\n",
      "\n",
      "Context: Dylan follows Melissa. Melissa follows Heather.\n",
      "P: Christian\n",
      "\n",
      "\n",
      "Context: Dylan follows Melissa. Heather is followed by Melissa.\n",
      "P: Anthony is preferred by Benjamin\n",
      "\n",
      "\n",
      "Context: Melissa is followed by Dylan. Melissa follows Heather.\n",
      "P: Dylan follows Melissa. Melissa follows Heather\n",
      "\n",
      "\n",
      "Context: Victoria supports Ethan. Ethan supports Jason.\n",
      "P: Heather\n",
      "\n",
      "\n",
      "Context: Victoria supports Ethan. Jason is supported by Ethan.\n",
      "P: Melissa is followed by Dylan\n",
      "\n",
      "\n",
      "Context: Victoria supports Ethan. Jason is supported by Ethan.\n",
      "P: Victoria supports Ethan. Ethan supports Jason\n",
      "\n",
      "\n",
      "Context: Natalie is deserved by Dylan. Natalie deserves Matthew.\n",
      "P: Victoria supports Ethan. Jason\n",
      "\n",
      "\n",
      "Context: Natalie is deserved by Dylan. Matthew is deserved by Natalie.\n",
      "P: Victoria supports Ethan\n",
      "\n",
      "\n",
      "28 - 236 - 55\n",
      "Context: Increased O\n",
      "P: Evangelical Lutheran Church\n",
      "\n",
      "\n",
      "28 - 812 - 1\n",
      "Context: Oxygen gas (O\n",
      "P: about 50% oxygen composition\n",
      "\n",
      "\n",
      "Context: Oxygen gas (O\n",
      "P: 50% oxygen composition\n",
      "\n",
      "\n",
      "28 - 1612 - 2\n",
      "Context: In the United States, the game was televised by CBS, as part of a cycle between the three main broadcast television partners of the NFL. The network's lead broadcast team of Jim Nantz and Phil Simms called the contest, with Tracy Wolfson and Evan Washburn on the sidelines. CBS introduced new features during the telecast, including pylon cameras and microphones along with EyeVision 360an array of 36 cameras along the upper deck that can be used to provide a 360-degree view of plays and \"bullet time\" effects. (An earlier version of EyeVision was last used in Super Bowl XXXV; for Super Bowl 50, the cameras were upgraded to 5K resolution.)\n",
      "P: 33\n",
      "\n",
      "\n",
      "Context: In the United States, the game was televised by CBS, as part of a cycle between the three main broadcast television partners of the NFL. The network's lead broadcast team of Jim Nantz and Christopher Price called the contest, with Tracy Wolfson and Evan Washburn on the sidelines. CBS introduced new features during the telecast, including pylon cameras and microphones along with EyeVision 360an array of 36 cameras along the upper deck that can be used to provide a 360-degree view of plays and \"bullet time\" effects. (An earlier version of EyeVision was last used in Super Bowl XXXV; for Super Bowl 50, the cameras were upgraded to 5K resolution.)\n",
      "P: Jim Nantz and Phil Simms\n",
      "\n",
      "\n",
      "Context: In the United States, the game was televised by CBS, as part of a cycle between the three main broadcast television partners of the NFL. The network's lead broadcast team of Jim Nantz and Phil Simms called the contest, with Tracy Wolfson and Joseph Long on the sidelines. CBS introduced new features during the telecast, including pylon cameras and microphones along with EyeVision 360an array of 36 cameras along the upper deck that can be used to provide a 360-degree view of plays and \"bullet time\" effects. (An earlier version of EyeVision was last used in Super Bowl XXXV; for Super Bowl 50, the cameras were upgraded to 5K resolution.)\n",
      "P: Jim Nantz and Christopher Price called the contest, with Tracy Wolfson and Evan Washburn\n",
      "\n",
      "\n",
      "Context: Since the election riots, the government and civil society organisations started programmes to avoid similar disasters in the future, said Agnes R. M. Aboum  executive director of TAABCO Research and Development Consultants in Nairobi  in the magazine D+C Development and Cooperation. For example, the Truth, Justice and Reconciliation Commission initiated community dialogues, the Evangelical Lutheran Church in Kenya started peace meetings and the Kenya National Dialogue and Reconciliation process was started.\n",
      "P: Jim Nantz and Phil Simms called the contest, with Tracy Wolfson and Joseph Long\n",
      "\n",
      "\n",
      "Context: By 1620 the Huguenots were on the defensive, and the government increasingly applied pressure. A series of three small civil wars known as the Huguenot rebellions broke out, mainly in southwestern France, between 1621 and 1629. revolted against royal authority. The uprising occurred a decade following the death of Henry IV, a Huguenot before converting to Catholicism, who had protected Protestants through the Edict of Nantes. His successor Louis XIII, under the regency of his Italian Catholic mother Marie de' Medici, became more intolerant of Protestantism. The Huguenots respond by establishing independent political and military structures, establishing diplomatic contacts with foreign powers, and openly revolting against central power. The rebellions were implacably suppressed by the French Crown.[citation needed]\n",
      "P: the Truth, Justice and Reconciliation Commission\n",
      "\n",
      "\n",
      "Context: By 1620 the Huguenots were on the defensive, and the government increasingly applied pressure. A series of three small civil wars known as the Huguenot rebellions broke out, mainly in southwestern France, between 1621 and 1629. revolted against royal authority. The uprising occurred a decade following the death of Michael Stewart, a Huguenot before converting to Catholicism, who had protected Protestants through the Edict of Nantes. His successor Louis XIII, under the regency of his Italian Catholic mother Marie de' Medici, became more intolerant of Protestantism. The Huguenots respond by establishing independent political and military structures, establishing diplomatic contacts with foreign powers, and openly revolting against central power. The rebellions were implacably suppressed by the French Crown.[citation needed]\n",
      "P: Henry IV\n",
      "\n",
      "\n",
      "Context: The Panthers used the San Jose State practice facility and stayed at the San Jose Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott.\n",
      "P: Louis XIII\n",
      "\n",
      "\n",
      "Context: The Panthers used the Coeur d'Alene State practice facility and stayed at the Coeur d'Alene Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott.\n",
      "P: San Jose State practice facility\n",
      "\n",
      "\n",
      "Context: The Panthers used the Strongsville State practice facility and stayed at the Strongsville Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott.\n",
      "P: Coeur d'Alene State\n",
      "\n",
      "\n",
      "Context: The Panthers used the San Jose State practice facility and stayed at the San Jose Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott.\n",
      "P: Strongsville\n",
      "\n",
      "\n",
      "Context: The Panthers used the Mount Vernon State practice facility and stayed at the Mount Vernon Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott.\n",
      "P: San Jose State practice facility\n",
      "\n",
      "\n",
      "Context: The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night. The event was held on February 1, 2016 at SAP Center in San Jose. Alongside the traditional media availabilities, the event featured an opening ceremony with player introductions on a replica of the Golden Gate Bridge.\n",
      "P: Mount Vernon State\n",
      "\n",
      "\n",
      "Context: The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night. The event was held on February 1, 2016 at SAP Center in Roanoke. Alongside the traditional media availabilities, the event featured an opening ceremony with player introductions on a replica of the Golden Gate Bridge.\n",
      "P: San Jose\n",
      "\n",
      "\n",
      "Context: The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night. The event was held on February 1, 2016 at SAP Center in Columbus. Alongside the traditional media availabilities, the event featured an opening ceremony with player introductions on a replica of the Golden Gate Bridge.\n",
      "P: SAP Center in Roanoke\n",
      "\n",
      "\n",
      "Context: Anna became a economist after Brittany did.\n",
      "P: SAP Center in Columbus\n",
      "\n",
      "\n",
      "Context: Sophia became a administrator after Noah did.\n",
      "P: Anna\n",
      "\n",
      "\n",
      "Context: Robert became a executive after Timothy did.\n",
      "P: Noah\n",
      "\n",
      "\n",
      "Context: Christian is not an accountant. Emily is.\n",
      "P: Robert became a executive after Timothy\n",
      "\n",
      "\n",
      "Context: Aaron is not an actor. Rachel is.\n",
      "P: Emily\n",
      "\n",
      "\n",
      "Context: Taylor is not an actress. Ryan is.\n",
      "P: Rachel\n",
      "\n",
      "\n",
      "Context: Michelle is an investigator. David is an adviser.\n",
      "P: Ryan\n",
      "\n",
      "\n",
      "Context: Adam is an accountant. Stephanie is a nurse.\n",
      "P: adviser\n",
      "\n",
      "\n",
      "Context: Adam is an executive. Taylor is an academic.\n",
      "P: Stephanie is a nurse\n",
      "\n",
      "\n",
      "Context: Lindsey and Angel are friends. He is an advisor, and she is an escort.\n",
      "P: Taylor\n",
      "\n",
      "\n",
      "Context: Taylor and Chad are friends. He is an author, and she is an accountant.\n",
      "P: Angel\n",
      "\n",
      "\n",
      "Context: Isaac and Amy are friends. Her mom is an attorney.\n",
      "P: Taylor\n",
      "\n",
      "\n",
      "Context: Amy and Isaac are friends. Her mom is an attorney.\n",
      "P: Isaac and Amy\n",
      "\n",
      "\n",
      "Context: Hailey and Angel are friends. Her mom is an interpreter.\n",
      "P: Isaac\n",
      "\n",
      "\n",
      "Context: Connor and Stephanie are friends. Her mom is an auditor.\n",
      "P: Angel\n",
      "\n",
      "\n",
      "Context: Christopher and Austin are friends. The former is an editor.\n",
      "P: Connor\n",
      "\n",
      "\n",
      "Context: Eric and David are friends. The former is an intern.\n",
      "P: Austin\n",
      "\n",
      "\n",
      "Context: Jessica and Ashley are friends. The former is an author.\n",
      "P: David\n",
      "\n",
      "\n",
      "Context: Michael is deserved by John.\n",
      "P: Ashley\n",
      "\n",
      "\n",
      "Context: Maria is deserved by Robert.\n",
      "P: Michael\n",
      "\n",
      "\n",
      "Context: Eric deserves Nicole.\n",
      "P: Maria\n",
      "\n",
      "\n",
      "Context: Jessica deserves Alexis. Ethan is deserved by Alexis.\n",
      "P: Nicole\n",
      "\n",
      "\n",
      "Context: William understands Megan. Megan understands Maria.\n",
      "P: Ethan\n",
      "\n",
      "\n",
      "Context: Sarah recognizes Thomas. Taylor is recognized by Thomas.\n",
      "P: Maria\n",
      "\n",
      "\n",
      "28 - 2188 - 38\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: A steady stream of oxygen gas is then produced by the exothermic reaction\n",
      "\n",
      "\n",
      "28 - 2956 - 1\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: exothermic reaction\n",
      "\n",
      "\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: A steady stream of oxygen gas is then produced by the exothermic reaction\n",
      "\n",
      "\n",
      "28 - 4364 - 2\n",
      "Context: Breathing pure O\n",
      "P: low total pressures used\n",
      "\n",
      "\n",
      "Context: Breathing pure O\n",
      "P: causes no damage due to the low total pressures used\n",
      "\n",
      "\n",
      "28 - 7052 - 2\n",
      "Context: For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.\n",
      "P: i/njuksl. The three-stage Saturn V was designed to send a fully fueled CSM and LM to the Moon. It was 33 feet (10.1 m) in diameter and stood 363 feet (110.6 m) tall with its 96,800-pound (43,900 kg) lunar payload. Its capability grew to 103,600 pounds (47,000 kg) for the later advanced lunar landings. The S-IC first stage burned RP-1/LOX for a rated thrust of 7,500,000 pounds-force (33,400 kN), which was upgraded to 7,610,000 pounds-force (33,900 kN). The second and third stages burned liquid hydrogen, and the third stage was a modified version of the S-IVB, with thrust increased to 230,000 lbf (1,020 kN) and capability to restart the engine for translunar injection after reaching a parking orbit.\n",
      "\n",
      "\n",
      "Context: For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.Sweden is internationally known for this innovative school voucher model that provides Swedish pupils with the opportunity to choose the school they prefer. \n",
      "P: stage was a modified version of the S-IVB, with\n",
      "\n",
      "\n",
      "Context: From their original homelands in Scandinavia and northern Europe, Germanic tribes expanded throughout northern and western Europe in the middle period of classical antiquity; southern Europe in late antiquity, conquering Celtic and other peoples; and by 800 CE, forming the Holy Bryan Empire, the first German Empire. However, there was no real systemic continuity from the Western Bryan Empire to its German successor which was famously described as \"not holy, not Bryan, and not an empire\", as a great number of small states and principalities existed in the loosely autonomous confederation. Although by 1000 CE, the Germanic conquest of central, western, and southern Europe (west of and including Italy) was complete, excluding only Muslim Iberia. There was, however, little cultural integration or national identity, and \"Germany\" remained largely a conceptual term referring to an amorphous area of central Europe.\n",
      "P: when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume\n",
      "\n",
      "\n",
      "Context: From their original homelands in Scandinavia and northern Europe, Germanic tribes expanded throughout northern and western Europe in the middle period of classical antiquity; southern Europe in late antiquity, conquering Celtic and other peoples; and by 800 CE, forming the Holy Ethan Empire, the first German Empire. However, there was no real systemic continuity from the Western Ethan Empire to its German successor which was famously described as \"not holy, not Ethan, and not an empire\", as a great number of small states and principalities existed in the loosely autonomous confederation. Although by 1000 CE, the Germanic conquest of central, western, and southern Europe (west of and including Italy) was complete, excluding only Muslim Iberia. There was, however, little cultural integration or national identity, and \"Germany\" remained largely a conceptual term referring to an amorphous area of central Europe.\n",
      "P: CE, forming the Holy Roman Empire, the first German Empire.\n",
      "\n",
      "\n",
      "Context: The image of the TARDIS has become firmly linked to the show in the public's consciousness; BBC scriptwriter Anthony Coburn, who lived in the resort of Herne Bay, Kent, was one of the people who conceived the idea of a police box as a time machine. In 1996, the BBC applied for a trade mark to use the TARDIS' blue police box design in merchandising associated with Doctor Who. In 1998, the Metropolitan Police Authority filed an objection to the trade mark claim; but in 2002, the Patent Office ruled in favour of the BBC.\n",
      "P: CE, forming the Holy Bryan Empire, the first German Empire. However, there\n",
      "\n",
      "\n",
      "Context: Apollo 5 (AS-204) was the first unmanned test flight of LM in Earth orbit, launched from pad 37 on January 22, 1968, by the Saturn IB that would have been used for Apollo 1. The LM engines were successfully test-fired and restarted, despite a computer programming error which cut short the first descent stage firing. The ascent engine was fired in abort mode, known as a \"fire-in-the-hole\" test, where it was lit simultaneously with jettison of the descent stage. Although Grumman wanted a second unmanned test, George Low decided the next LM flight would be manned.\n",
      "P: TARDIS' blue police box design in merchandising associated with Doctor Who. In 1998,\n",
      "\n",
      "\n",
      "Context: Apollo 5 (AS-204) was the first unmanned test flight of LM in Earth orbit, launched from pad 37 on January 22, 1968, by the Saturn IB that would have been used for Apollo 1. The LM engines were successfully test-fired and restarted, despite a computer programming error which cut short the first descent stage firing. The ascent engine was fired in abort mode, known as a \"fire-in-the-hole\" test, where it was lit simultaneously with jettison of the descent stage. Although Grumman wanted a second unmanned test, Michael Smith decided the next LM flight would be manned.\n",
      "P: TARDIS' blue police box design in merchandising associated with Doctor Who. In 1998,\n",
      "\n",
      "\n",
      "Context: The Amazon rainforest (Portuguese: Floresta Amaznica or Amaznia; Spanish: Selva Amaznica, Amazona or usually Amazonia; French: Fort amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
      "P: January 22, 1968,\n",
      "\n",
      "\n",
      "Context: The Amazon rainforest (Portuguese: Floresta Amaznica or Amaznia; Spanish: Selva Amaznica, Amazona or usually Amazonia; French: Fort amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Kazakhstan with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
      "P: January 22, 1968, by the Saturn IB that would have been used for Apollo 1.\n",
      "\n",
      "\n",
      "Context: Established originally by the Massachusetts legislature and soon thereafter named for John Harvard (its first benefactor), Harvard is the United States' oldest institution of higher learning, and the Harvard Corporation (formally, the President and Fellows of Harvard College) is its first chartered corporation. Although never formally affiliated with any denomination, the early College primarily trained Congregationalist and Unitarian clergy. Its curriculum and student body were gradually secularized during the 18th century, and by the 19th century Harvard had emerged as the central cultural establishment among Boston elites. Following the American Civil War, President Charles W. Eliot's long tenure (18691909) transformed the college and affiliated professional schools into a modern research university; Harvard was a founding member of the Association of American Universities in 1900. James Bryant Conant led the university through the Great Depression and World War II and began to reform the curriculum and liberalize admissions after the war. The undergraduate college became coeducational after its 1977 merger with Radcliffe College.\n",
      "P: 7,000,000 square kilometres (2,700,000 sq mi), of which\n",
      "\n",
      "\n",
      "Context: Established originally by the Massachusetts legislature and soon thereafter named for John Harvard (its first benefactor), Harvard is the United States' oldest institution of higher learning, and the Harvard Corporation (formally, the President and Fellows of Harvard College) is its first chartered corporation. Although never formally affiliated with any denomination, the early College primarily trained Congregationalist and Unitarian clergy. Its curriculum and student body were gradually secularized during the 18th century, and by the 19th century Harvard had emerged as the central cultural establishment among Campbell elites. Following the American Civil War, President Charles W. Eliot's long tenure (18691909) transformed the college and affiliated professional schools into a modern research university; Harvard was a founding member of the Association of American Universities in 1900. James Bryant Conant led the university through the Great Depression and World War II and began to reform the curriculum and liberalize admissions after the war. The undergraduate college became coeducational after its 1977 merger with Radcliffe College.\n",
      "P: Fort amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or\n",
      "\n",
      "\n",
      "Context: The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 134849, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.[citation needed]\n",
      "P: by the\n",
      "\n",
      "\n",
      "Context: The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Slovenia, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 134849, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.[citation needed]\n",
      "P: by the\n",
      "\n",
      "\n",
      "Context: Julian is not a CEO, Stephanie is.\n",
      "P: 47, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the\n",
      "\n",
      "\n",
      "Context: Jeremiah is not a CEO, Julia is.\n",
      "P: Sidon, Damascus, Homs, and Aleppo. In 134849, the disease reached Antioch.\n",
      "\n",
      "\n",
      "Context: Anthony notices Robert.\n",
      "P: by Madison\n",
      "\n",
      "\n",
      "Context: Kimberly accepts Jordan. Jordan accepts Abigail.\n",
      "P: by\n",
      "\n",
      "\n",
      "29 - 6 - 17\n",
      "Context: Uptake of O\n",
      "P: respiration, so oxygen supplementation is used in medicine. Treatment not only increases oxygen levels in the patient\n",
      "\n",
      "\n",
      "29 - 8 - 1\n",
      "Context: Oxygen, as a supposed mild euphoric, has a history of recreational use in oxygen bars and in sports. Oxygen bars are establishments, found in Japan, California, and Las Vegas, Nevada since the late 1990s that offer higher than normal O\n",
      "P: 1990s that offer higher than normal O\\n2 exposure for a fee. Professional athletes, especially in American football, also sometimes\n",
      "\n",
      "\n",
      "29 - 12 - 1\n",
      "Context: Due to its electronegativity, oxygen forms chemical bonds with almost all other elements to give corresponding oxides. The surface of most metals, such as aluminium and titanium, are oxidized in the presence of air and become coated with a thin film of oxide that passivates the metal and slows further corrosion. Many oxides of the transition metals are non-stoichiometric compounds, with slightly less metal than the chemical formula would show. For example, the mineral FeO (wstite) is written as Fe\n",
      "P: Fe\\n1  xO\n",
      "\n",
      "\n",
      "29 - 52 - 1\n",
      "Context: alcohols (R-OH); ethers (R-O-R); ketones (R-CO-R); aldehydes (R-CO-H); carboxylic acids (R-COOH); esters (R-COO-R); acid anhydrides (R-CO-O-CO-R); and amides (R-C(O)-NR\n",
      "P: 80% interest in cable sports channel ESPN\n",
      "\n",
      "\n",
      "29 - 68 - 1\n",
      "Context: Breathing pure O\n",
      "P: low total pressures used\n",
      "\n",
      "\n",
      "Context: Breathing pure O\n",
      "P: the low total pressures used\n",
      "\n",
      "\n",
      "29 - 268 - 2\n",
      "Context: Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O\n",
      "P: global downward\n",
      "\n",
      "\n",
      "Context: Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O\n",
      "P: global downward trend\n",
      "\n",
      "\n",
      "29 - 348 - 2\n",
      "Context: This is when chloroplast constriction begins.\n",
      "P: balance of parties across Parliament\n",
      "\n",
      "\n",
      "29 - 428 - 1\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds\n",
      "\n",
      "\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen\n",
      "\n",
      "\n",
      "29 - 716 - 2\n",
      "Context: Oxygen gas (O\n",
      "P: about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O\\n2 partial pressure of about 21 kPa\n",
      "\n",
      "\n",
      "Context: Oxygen gas (O\n",
      "P: 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O\\n2 partial pressure of about 21 kPa\n",
      "\n",
      "\n",
      "29 - 972 - 2\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds\n",
      "\n",
      "\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds, in particular various complex silicates\n",
      "\n",
      "\n",
      "29 - 1004 - 2\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds\n",
      "\n",
      "\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds, in particular various complex silicates\n",
      "\n",
      "\n",
      "29 - 4364 - 2\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds\n",
      "\n",
      "\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds, in particular various complex silicates\n",
      "\n",
      "\n",
      "29 - 5388 - 2\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: chemical oxygen generators\n",
      "\n",
      "\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: A steady stream of oxygen gas is then produced by the exothermic reaction\n",
      "\n",
      "\n",
      "29 - 6540 - 2\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: exothermic reaction\n",
      "\n",
      "\n",
      "Context: People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n",
      "P: chemical oxygen generators\n",
      "\n",
      "\n",
      "Context: Oxygen gas (O\n",
      "P: 50% oxygen composition at standard pressure\n",
      "\n",
      "\n",
      "Context: Oxygen gas (O\n",
      "P: 50% oxygen composition\n",
      "\n",
      "\n",
      "29 - 7052 - 4\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds\n",
      "\n",
      "\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: various complex silicates\n",
      "\n",
      "\n",
      "29 - 7180 - 2\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: oxygen compounds\n",
      "\n",
      "\n",
      "Context: Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n",
      "P: various complex silicates\n",
      "\n",
      "\n",
      "29 - 7820 - 2\n"
     ]
    }
   ],
   "source": [
    "for seed in [27, 28, 29]:\n",
    "    for chkpt in ALL_CHECKPOINTS:\n",
    "        with open(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{chkpt}/checklist_results.txt', 'r') as f:\n",
    "            summary = f.read()\n",
    "\n",
    "        contexts = re.findall(r\"[\\n\\r].*C:\\s*([^\\n\\r]*)\", summary)\n",
    "        preds = re.findall(r\"[\\n\\r].*P:\\s*([^\\n\\r]*)\", summary)\n",
    "#         assert len(contexts) == len(preds)\n",
    "\n",
    "        count = 0\n",
    "        affected_checkpoints = []\n",
    "        for c, p in zip(contexts, preds):\n",
    "            if p =='empty':\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    assert p in c\n",
    "#                     assert '\\n' not in p\n",
    "                except:\n",
    "                    count += 1 \n",
    "                    print(\"Context:\", c)\n",
    "                    print(\"P:\", p)\n",
    "                    print('\\n')\n",
    "\n",
    "        if count > 0:\n",
    "            print(seed, '-', chkpt, '-', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "20191f6a-8a63-4053-bd5d-102895759619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oxygen gas (O\\n2) can be toxic at elevated partial pressures, leading to convulsions and other health problems.[j] Oxygen toxicity usually begins to occur at partial pressures more than 50 kilopascals (kPa), equal to about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O\\n2 partial pressure of about 21 kPa. This is not a problem except for patients on mechanical ventilators, since gas supplied through oxygen masks in medical applications is typically composed of only 30%50% O\\n2 by volume (about 30 kPa at standard pressure). (although this figure also is subject to wide variation, depending on type of mask).'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in context_list if 'Oxygen gas (' in w][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ffb29f-2940-4549-99b6-2ce2ad3686fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31b1ad19-476d-427e-8a44-3fea91a67148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 151)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25298b5c-199f-4086-8ea5-500f21e7260f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emily is older than Nicholas',\n",
       " 'Michael is older than Natalie',\n",
       " 'Lauren is greater than Richard',\n",
       " 'Justin',\n",
       " 'Emily',\n",
       " 'Emily is enthusiastic about the project. Justin',\n",
       " 'Abigail is really positive about the project. Jordan',\n",
       " 'Abigail is really positive about the project. Jordan',\n",
       " 'Abigail',\n",
       " 'Laura',\n",
       " 'red',\n",
       " 'new blue',\n",
       " 'new and blue',\n",
       " 'old and blue',\n",
       " 'American executive',\n",
       " 'Japanese architect',\n",
       " 'Bangladeshi auditor',\n",
       " 'tractor',\n",
       " 'tractor',\n",
       " 'tractor',\n",
       " 'Jeffrey bought a tractor. Jose',\n",
       " 'Laura bought a snake. Kimberly',\n",
       " 'Kimberly',\n",
       " 'Emily',\n",
       " 'Emily',\n",
       " 'Jennifer is worse than Victoria',\n",
       " 'Aaron is smarter than Daniel',\n",
       " 'Daniel is dumber than Aaron',\n",
       " 'Amy is better than Ryan',\n",
       " 'Ryan is worse than Amy',\n",
       " 'Sarah is more visible than Kelly',\n",
       " 'Sarah is more visible than Kelly',\n",
       " 'Sarah is more visible than Kelly',\n",
       " 'Eric is more unhappy than Danielle',\n",
       " 'Danielle',\n",
       " 'Eric',\n",
       " 'Jacob is more cautious than Ryan',\n",
       " 'Ryan',\n",
       " 'Ryan',\n",
       " '22',\n",
       " 'chorale cantatas',\n",
       " 'immunization',\n",
       " 'over-fishing and long-term environmental changes',\n",
       " 'i/njuksl pn tan/; Locally: i/njuksl pn tan/), commonly known as Newcastle, is a city in Tyne and Wear, North East England, 103 miles (166 km) south of Edinburgh and 277 miles (446 km) north of London on the northern bank of the River Tyne, 8.5 mi (13.7 km) from the North Sea. Newcastle is the most populous city in the North East and Tyneside the eighth most populous conurbation in the United Kingdom. Newcastle is a member of the English Core Cities Group and is a member of the Eurocities network of European cities. Newcastle was part of the county of Northumberland until 1400, when it became a county itself, a status it retained until becoming part of Tyne and Wear in 1974.[not in citation given] The regional nickname and dialect for people from Newcastle and the surrounding area is Geordie.',\n",
       " 'oxygen compounds, in particular various complex silicates',\n",
       " 'i/njuksl pn tan/; Locally: i/njuksl pn tan/), commonly known as Newcastle, is a city in Tyne and Wear, North East England, 103 miles (166 km) south of Edinburgh and 277 miles (446 km) north of London on the northern bank of the River Tyne, 8.5 mi (13.7 km) from the North Sea. Newcastle is the most populous city in the North East and Tyneside the eighth most populous conurbation in the United Kingdom. Newcastle is a member of the English Core Cities Group and is a member of the Eurocities network of European cities. Newcastle was part of the county of Northumberland until 1400, when it became a county itself, a status it retained until becoming part of Tyne and Wear in 1974.[not in citation given] The regional nickname and dialect for people from Newcastle and the surrounding area is Geordie.',\n",
       " '103',\n",
       " 'Paulinella chromatophora is an exception that acquired a photosynthetic cyanobacterial endosymbiont more recently',\n",
       " 'NASA',\n",
       " 'starch',\n",
       " 'segregation academies',\n",
       " 'through various associations and other arrangements',\n",
       " 'double displacement loop',\n",
       " 'Saudi',\n",
       " '25m people',\n",
       " '25m people',\n",
       " 'matrices',\n",
       " '14th century',\n",
       " '14th century',\n",
       " 'Christchurch',\n",
       " 'typhus, smallpox and respiratory infections',\n",
       " 'typhus, smallpox and respiratory infections',\n",
       " 'John Williams',\n",
       " 'Ismail El Gizouli',\n",
       " 'Ismail El Gizouli',\n",
       " 'a new entrance building',\n",
       " 'eight',\n",
       " 'eight',\n",
       " 'Small Catechism',\n",
       " 'McGann and Eccleston',\n",
       " 'McGann and Eccleston',\n",
       " 'Britain',\n",
       " 'South African Schools Act of 1996',\n",
       " 'South African Schools Act of 1996',\n",
       " 'three',\n",
       " 'Duisburg',\n",
       " 'Duisburg',\n",
       " 'every four years',\n",
       " 'Denver',\n",
       " 'Saginaw',\n",
       " 'Jordan',\n",
       " 'George',\n",
       " 'Melissa is not a nurse, Luis',\n",
       " 'Nathan',\n",
       " 'Kevin',\n",
       " 'Christian',\n",
       " 'Emily',\n",
       " 'Richard',\n",
       " 'Jessica',\n",
       " 'Sophia',\n",
       " 'Jordan',\n",
       " 'Kyle',\n",
       " 'Isabella',\n",
       " 'Amy',\n",
       " 'Jordan',\n",
       " 'Kevin',\n",
       " 'Jacob',\n",
       " 'Stephanie',\n",
       " 'Steven',\n",
       " 'Emma',\n",
       " 'Sarah',\n",
       " 'Christina',\n",
       " 'Sara',\n",
       " 'Christina',\n",
       " 'Matthew',\n",
       " 'Maria',\n",
       " 'Austin is an editor',\n",
       " 'Kimberly',\n",
       " 'Jennifer',\n",
       " 'Austin is an investigator',\n",
       " 'Amanda',\n",
       " 'Brandon',\n",
       " 'Jordan',\n",
       " 'She',\n",
       " 'Charles and Amber are friends. He',\n",
       " 'she',\n",
       " 'She',\n",
       " 'He',\n",
       " 'she',\n",
       " 'Cynthia and Cameron are friends. She',\n",
       " 'He',\n",
       " 'Christine',\n",
       " 'His',\n",
       " 'His',\n",
       " 'His',\n",
       " 'Aaron',\n",
       " 'Benjamin and Joshua',\n",
       " 'Elizabeth',\n",
       " 'Jordan and Victoria',\n",
       " 'Jordan and Victoria',\n",
       " 'Jacob',\n",
       " 'Jamie',\n",
       " 'Jamie and Jason',\n",
       " 'Jacob',\n",
       " 'Scott',\n",
       " 'Patrick',\n",
       " 'William',\n",
       " 'Patrick accepts Madison',\n",
       " 'Madison',\n",
       " 'Isabella',\n",
       " 'Amy trusts Taylor',\n",
       " 'Taylor',\n",
       " 'Madison',\n",
       " 'Andrea',\n",
       " 'Jessica',\n",
       " 'Stephanie',\n",
       " 'Michelle',\n",
       " 'Heather',\n",
       " 'Benjamin',\n",
       " 'Olivia',\n",
       " 'Amy']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfe2b9-8980-4787-8b19-484772634691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1730a26f-562f-4d7b-82b4-8d2591b94eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [27, 28, 29]:\n",
    "    for chkpt in ALL_CHECKPOINTS:\n",
    "        num_lines = sum(1 for line in open(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{chkpt}/predict_predictions_flat.txt'))\n",
    "        assert num_lines == 71293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb16459-97a6-4540-80ca-2ce1cb931f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13881c26-bffa-4ac7-8f7c-4f076e19048f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ff5b6-3f6c-4944-929d-3030b9360df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2bfc16-cd10-4c4d-9065-4cbdf24810c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714c8aaf-2feb-403d-a9e4-2c6e4b98d49d",
   "metadata": {},
   "source": [
    "## Check if predictions in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c5b8845-5708-4bb3-8018-a3c08f6824f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import checklist\n",
    "from checklist.test_suite import TestSuite\n",
    "from tqdm.contrib.itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b0c7a4-81e4-457a-9554-374d7ce861a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6751118e-4e22-4f41-a41b-10757cae61c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEEDS = [27, 28, 29]\n",
    "ALL_CHECKPOINTS = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32, 36, 44, 52, 60, 68, 76, 92, 108, 124, 140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348, 364, 380, 396, 428, 460, 492, 524, 556, 588, 620, 652, 684, 716, 748, 780, 812, 844, 876, 908, 940, 972, 1004, 1036, 1100, 1164, 1228, 1292, 1356, 1420, 1484, 1548, 1612, 1676, 1804, 1932, 2060, 2188, 2316, 2444, 2572, 2700, 2828, 2956, 3084, 3212, 3340, 3468, 3596, 3724, 3852, 3980, 4108, 4236, 4364, 4492, 4620, 4748, 4876, 5004, 5132, 5260, 5388, 5516, 5644, 5772, 5900, 6028, 6156, 6284, 6412, 6540, 6668, 6796, 6924, 7052, 7180, 7308, 7436, 7564, 7692, 7820, 7948]\n",
    "len(ALL_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398925c2-a490-4d55-8cc2-f7026ee2eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_path = '/home/sgeorge/Github/checklist/release_data/squad/squad_suite.pkl'\n",
    "suite = TestSuite.from_file(suite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1e1b05-af5f-418e-b353-9e29cbf74338",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [x[0] for x in [ast.literal_eval(x) for x in suite.get_raw_examples()]]\n",
    "question_list = [x[1] for x in [ast.literal_eval(x) for x in suite.get_raw_examples()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a32684-9df9-4a41-95e1-65d3d6d13192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71293"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8496edf0-14bf-467a-8edc-ef0abf3ba56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Christian is greater than Tiffany.',\n",
       " 'Christian is greater than Tiffany.',\n",
       " 'Olivia is stranger than William.',\n",
       " 'Olivia is stranger than William.',\n",
       " 'Abigail is stronger than Jonathan.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e955bc7-566b-4e8d-a1ac-eca06b571914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who is less great?',\n",
       " 'Who is greater?',\n",
       " 'Who is less strange?',\n",
       " 'Who is stranger?',\n",
       " 'Who is less strong?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b53928-4617-4d82-930e-1dbce19e22f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=360.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for seed, chkpt in product(SEEDS, ALL_CHECKPOINTS):\n",
    "    \n",
    "    with open(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed={seed}/checkpoint-{chkpt}/predict_predictions.json') as json_file:\n",
    "        predictions = json.load(json_file)\n",
    "\n",
    "    for context, pred in zip(context_list, predictions.values()):\n",
    "        if pred == 'empty':\n",
    "            continue\n",
    "        else:\n",
    "            count += 1\n",
    "            assert pred in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e802beaf-89b4-4ba8-a413-1ca32a02a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.run_from_file('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions.json', overwrite=True, file_format='pred_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a4ef57-79b0-4987-abb4-4d6c15a523d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# suite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143daa-758a-480d-a2b2-d195e40e2b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bba29bee-0bb9-47a1-ba6e-b336808aa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions.json') as json_file:\n",
    "    predictions = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a2eb8d7-fb39-4f18-a016-baa969206996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71293"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_preds = list(predictions.values())\n",
    "len(flat_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70ca9e50-b498-43dc-9fbd-b19b94d6010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71293, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  empty\n",
       "1  empty\n",
       "2  empty\n",
       "3  empty\n",
       "4  empty"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_preds_df = pd.DataFrame(flat_preds)\n",
    "print(flat_preds_df.shape)\n",
    "flat_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552cb63-d486-4a8f-b202-1a004df38fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3cd220a-c6aa-45c2-a955-faa435f3e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71293, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>empty\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>empty\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>empty\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>empty\\r\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  empty\\r\\n\n",
       "1  empty\\r\\n\n",
       "2  empty\\r\\n\n",
       "3  empty\\r\\n\n",
       "4  empty\\r\\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_preds_df = pd.DataFrame([x + '\\r\\n' for x in flat_preds])\n",
    "print(flat_preds_df.shape)\n",
    "flat_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "28ccce1c-a883-428e-bcc4-5010e4db610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of silicon (silica SiO\\n2, as found in granite and quartz), aluminium\n"
     ]
    }
   ],
   "source": [
    "print('of silicon (silica SiO\\\\n2, as found in granite and quartz), aluminium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "92c13774-54c4-4916-911e-480b1a37df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat_new.txt', 'w') as f:\n",
    "    f.writelines(s + '\\n' for s in [x.replace('\\n', '\\\\n') for x in flat_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdf531-ab75-4dd4-afbd-42ee74d05268",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.replace('\\n', '\\\\n') for x in flat_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc250a1-fc8b-4848-8503-7196c3580ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5f83f3a-7f43-4cde-a7cc-58e52335c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_preds_df.to_csv('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat_new.csv', header=False, index=False, line_terminator='\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc85f48b-9b16-4040-a7dd-97b539d5148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat_new.txt','w') as log:\n",
    "    for value in flat_preds:\n",
    "        log.write('{}\\n'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72b9946d-17d0-4d55-b3fe-7a0af3f174bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of silicon (silica SiO\\n2, as found in granite and quartz), aluminium',\n",
       " 'of silicon (silica SiO\\n2, as found in granite and quartz), aluminium',\n",
       " 'sometimes have supplemental O\\n2 supplies.[h]',\n",
       " 'sometimes have supplemental O\\n2 supplies.[h]']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in flat_preds if '\\n' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ef602a8-328f-448b-ac21-e7898ffdfe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\\n2). The Earth's crustal rock is composed in large part of oxides of silicon (silica SiO\\n2, as found in granite and quartz), aluminium (aluminium oxide Al\\n2O\\n3, in bauxite and corundum), iron (iron(III) oxide Fe\\n2O\\n3, in hematite and rust), and calcium carbonate (in limestone). The rest of the Earth's crust is also made of oxygen compounds, in particular various complex silicates (in silicate minerals). The Earth's mantle, of much larger mass than the crust, is largely composed of silicates of magnesium and iron.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list[30426]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bcc80-354d-4c8e-bfc0-c1fea28cad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0fc7744-13b1-429a-b609-6b39296de3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70787, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat.txt', delimiter = \"\\t\", header=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc150568-6867-49e4-83e3-3deafba057a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71293"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b6c2a09-bc52-497d-a3f0-b627f2865826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71293, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat_new.txt', delimiter = \"\\t\", header=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8613244e-22b5-4d48-a660-85346dcd6622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71293, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat_new.txt', delimiter = \"\\t\", header=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d7efb-883a-4f65-bf46-1988bfb43d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5436bb9b-e933-4196-842d-9e79aec6db18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'empty', 'empty', ..., 'empty', 'empty', 'empty'],\n",
       "      dtype='<U194')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70ea5426-900d-4ab0-b31c-2c31f487bd45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\n(shapes (71293,), (70787,) mismatch)\n x: array(['empty', 'empty', 'empty', ..., 'empty', 'empty', 'empty'],\n      dtype='<U194')\n y: array(['empty', 'empty', 'empty', ..., 'empty', 'empty', 'empty'],\n      dtype=object)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9909c8297521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rclearn/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    759\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mflagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not equal\n\n(shapes (71293,), (70787,) mismatch)\n x: array(['empty', 'empty', 'empty', ..., 'empty', 'empty', 'empty'],\n      dtype='<U194')\n y: array(['empty', 'empty', 'empty', ..., 'empty', 'empty', 'empty'],\n      dtype=object)"
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(np.array(list(predictions.values())), pd.read_csv('predictions/checklist/albert-xlarge-v2-squadv1-adversarialall-wu=100-lr=3e5-bs=32-msl=384-seed=27/checkpoint-8/predict_predictions_flat.txt', delimiter = \"\\t\", header=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab592f-9628-42bb-80fc-cc6bcca52652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
