#!/bin/bash -l

#$ -cwd
#$ -S /bin/bash
#$ -l tmem=32G
#$ -l h_vmem=32G
#$ -t 1-1
#$ -l h_rt=72:00:00
#$ -o /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/array.out
#$ -e /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/array.err
#$ -l gpu=true
#$ -l gpu_p100=yes

hostname
date

# Activate conda environment
conda activate rclearn

export LANG="en_US.utf8"
export LANGUAGE="en_US:en"
export WANDB_PROJECT="albert_squadv2"

cd /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn

test $SGE_TASK_ID -eq 1 && sleep 10 && python src/models/practice_finetune/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 24 \
    --learning_rate 3e-5 \
    --max_seq_length 512 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/outputs \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 815 1630 2445 3260 4075 4890 5705 6520 7335 \
    --save_strategy steps \
    --report_to wandb \
    --run_name main-run \
    --max_steps 8144 \
    --warmup_steps 814 \
    --version_2_with_negative \
    --logging_steps 815

date