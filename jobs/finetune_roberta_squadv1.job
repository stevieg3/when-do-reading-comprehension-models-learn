#!/bin/bash -l

#$ -cwd
#$ -S /bin/bash
#$ -l tmem=32G
#$ -l h_vmem=32G
#$ -t 1-1
#$ -l h_rt=72:00:00
#$ -o $HOME/Github/when-do-reading-comprehension-models-learn/array.out
#$ -e $HOME/Github/when-do-reading-comprehension-models-learn/array.err
#$ -l gpu=true
#$ -l gpu_p100=yes

hostname
date

# Activate conda environment
conda activate rclearn

export LANG="en_US.utf8"
export LANGUAGE="en_US:en"
export WANDB_PROJECT="finetune_roberta_squadv1"

cd $HOME/Github/when-do-reading-comprehension-models-learn

test $SGE_TASK_ID -eq 1 && sleep 10 && python src/models/practice_finetune/run_qa.py \
    --model_name_or_path roberta-large \
    --dataset_name squad \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 24 \
    --learning_rate 1.5e-5 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir $HOME/Github/when-do-reading-comprehension-models-learn/outputs \
    --overwrite_output_dir \
    --overwrite_cache \
    --logging_steps 1000 \
    --evaluation_strategy steps \
    --report_to wandb \
    --run_name main-run \
    --save_strategy epoch \
    --num_train_epochs 2 \
    --warmup_ratio 0.06 \
    --weight_decay 0.01

date