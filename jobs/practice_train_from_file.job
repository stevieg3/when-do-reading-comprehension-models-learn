#!/bin/bash -l

#$ -cwd
#$ -S /bin/bash
#$ -l tmem=15G
#$ -l h_vmem=15G
#$ -t 1-1
#$ -l h_rt=4:00:00
#$ -o $HOME/Github/when-do-reading-comprehension-models-learn/array.out
#$ -e $HOME/Github/when-do-reading-comprehension-models-learn/array.err
#$ -l gpu=true

hostname
date

# Activate conda environment
conda activate rclearn

export LANG="en_US.utf8"
export LANGUAGE="en_US:en"
export WANDB_PROJECT="test_run"

cd $HOME/Github/when-do-reading-comprehension-models-learn

test $SGE_TASK_ID -eq 1 && sleep 10 && python src/models/practice_finetune/run_qa.py \
    --model_name_or_path roberta-large \
    --do_train \
    --do_eval \
    --learning_rate 1.5e-5 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir $HOME/Github/when-do-reading-comprehension-models-learn/temp_outputs \
    --overwrite_output_dir \
    --overwrite_cache \
    --logging_steps 10 \
    --evaluation_strategy epoch \
    --report_to wandb \
    --run_name main-run \
    --save_strategy epoch \
    --weight_decay 0.01 \
    --gradient_accumulation_steps 1 \
    --per_device_train_batch_size 1 \
    --max_train_samples 128 \
    --max_eval_samples 64 \
    --num_train_epochs 2 \
    --train_file $HOME/Github/when-do-reading-comprehension-models-learn/data/processed/squad1_dbert_2_3_weighted_train.json \
    --validation_file $HOME/Github/when-do-reading-comprehension-models-learn/data/processed/squad1_dbert_2_3_weighted_dev.json

date
