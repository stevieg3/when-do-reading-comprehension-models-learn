#!/bin/bash -l

#$ -cwd
#$ -S /bin/bash
#$ -l tmem=16G
#$ -t 1-6
#$ -l h_rt=72:00:00
#$ -o /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/array.out
#$ -e /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/array.err
#$ -l gpu=true
#$ -l gpu_p100=yes
#$ -pe gpu 2
#$ -R y

hostname
date

# Activate conda environment
conda activate rclearn

export LANG="en_US.utf8"
export LANGUAGE="en_US:en"
export WANDB_PROJECT="albert-xlarge-v2"

cd /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn

test $SGE_TASK_ID -eq 1 && sleep 10 && python -m torch.distributed.launch --nproc_per_node=2 src/models/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --version_2_with_negative \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-5 \
    --max_seq_length 384 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/models/albertxlargev2-squadv2-warmup100-seed27 \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 1 2 4 8 16 32 64 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 5248 5376 5504 5632 5760 5888 6016 6144 6272 6400 6528 6656 6784 6912 7040 7168 7296 7424 7552 7680 7808 7936 8064 8192 \
    --save_strategy steps \
    --report_to wandb \
    --seed 27 \
    --run_name squadv2-warmup100-seed27 \
    --max_steps 8200 \
    --warmup_steps 100 \
    --fp16 True \
    --logging_steps 128 \
    > logs/albertxlargev2-squadv2-warmup100-seed27.log 2>&1

test $SGE_TASK_ID -eq 2 && sleep 10 && python -m torch.distributed.launch --nproc_per_node=2 src/models/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --version_2_with_negative \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-5 \
    --max_seq_length 384 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/models/albertxlargev2-squadv2-warmup100-seed28 \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 1 2 4 8 16 32 64 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 5248 5376 5504 5632 5760 5888 6016 6144 6272 6400 6528 6656 6784 6912 7040 7168 7296 7424 7552 7680 7808 7936 8064 8192 \
    --save_strategy steps \
    --report_to wandb \
    --seed 28 \
    --run_name squadv2-warmup100-seed28 \
    --max_steps 8200 \
    --warmup_steps 100 \
    --fp16 True \
    --logging_steps 128 \
    > logs/albertxlargev2-squadv2-warmup100-seed28.log 2>&1

test $SGE_TASK_ID -eq 3 && sleep 10 && python -m torch.distributed.launch --nproc_per_node=2 src/models/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --version_2_with_negative \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-5 \
    --max_seq_length 384 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/models/albertxlargev2-squadv2-warmup100-seed29 \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 1 2 4 8 16 32 64 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 5248 5376 5504 5632 5760 5888 6016 6144 6272 6400 6528 6656 6784 6912 7040 7168 7296 7424 7552 7680 7808 7936 8064 8192 \
    --save_strategy steps \
    --report_to wandb \
    --seed 29 \
    --run_name squadv2-warmup100-seed29 \
    --max_steps 8200 \
    --warmup_steps 100 \
    --fp16 True \
    --logging_steps 128 \
    > logs/albertxlargev2-squadv2-warmup100-seed29.log 2>&1

test $SGE_TASK_ID -eq 4 && sleep 10 && python -m torch.distributed.launch --nproc_per_node=2 src/models/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --version_2_with_negative \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-5 \
    --max_seq_length 384 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/models/albertxlargev2-squadv2-warmup0-seed27 \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 1 2 4 8 16 32 64 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 5248 5376 5504 5632 5760 5888 6016 6144 6272 6400 6528 6656 6784 6912 7040 7168 7296 7424 7552 7680 7808 7936 8064 8192 \
    --save_strategy steps \
    --report_to wandb \
    --seed 27 \
    --run_name squadv2-warmup0-seed27 \
    --max_steps 8200 \
    --warmup_steps 0 \
    --fp16 True \
    --logging_steps 128 \
    > logs/albertxlargev2-squadv2-warmup0-seed27.log 2>&1

test $SGE_TASK_ID -eq 5 && sleep 10 && python -m torch.distributed.launch --nproc_per_node=2 src/models/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --version_2_with_negative \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-5 \
    --max_seq_length 384 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/models/albertxlargev2-squadv2-warmup0-seed28 \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 1 2 4 8 16 32 64 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 5248 5376 5504 5632 5760 5888 6016 6144 6272 6400 6528 6656 6784 6912 7040 7168 7296 7424 7552 7680 7808 7936 8064 8192 \
    --save_strategy steps \
    --report_to wandb \
    --seed 28 \
    --run_name squadv2-warmup0-seed28 \
    --max_steps 8200 \
    --warmup_steps 0 \
    --fp16 True \
    --logging_steps 128 \
    > logs/albertxlargev2-squadv2-warmup0-seed28.log 2>&1

test $SGE_TASK_ID -eq 6 && sleep 10 && python -m torch.distributed.launch --nproc_per_node=2 src/models/run_qa.py \
    --model_name_or_path albert-xlarge-v2 \
    --dataset_name squad_v2 \
    --version_2_with_negative \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-5 \
    --max_seq_length 384 \
    --output_dir /cluster/project7/max_harderqs/projects/sgeorge/when-do-reading-comprehension-models-learn/models/albertxlargev2-squadv2-warmup0-seed29 \
    --overwrite_output_dir \
    --overwrite_cache \
    --evaluation_strategy steps \
    --save_steps_schedule 1 2 4 8 16 32 64 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 5248 5376 5504 5632 5760 5888 6016 6144 6272 6400 6528 6656 6784 6912 7040 7168 7296 7424 7552 7680 7808 7936 8064 8192 \
    --save_strategy steps \
    --report_to wandb \
    --seed 29 \
    --run_name squadv2-warmup0-seed29 \
    --max_steps 8200 \
    --warmup_steps 0 \
    --fp16 True \
    --logging_steps 128 \
    > logs/albertxlargev2-squadv2-warmup0-seed29.log 2>&1

date
